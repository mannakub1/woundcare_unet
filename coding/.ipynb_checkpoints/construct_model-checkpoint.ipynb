{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU[0] Tesla M60, CPU)\n"
     ]
    }
   ],
   "source": [
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "import cntk_unet\n",
    "from cntk.learners import learning_rate_schedule, UnitType\n",
    "from cntk.device import try_set_default_device, gpu, all_devices\n",
    "print(all_devices())\n",
    "try_set_default_device(gpu(0))\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix the random seed so that LR examples are repeatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import cntk\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from cntk.initializer import normal\n",
    "global f \n",
    "global index_model\n",
    "index_model = 1\n",
    "model_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "from cntk.layers import Convolution, MaxPooling, Dense\n",
    "from cntk.initializer import glorot_uniform\n",
    "from cntk.ops import relu, sigmoid, input_variable\n",
    "\n",
    "def UpSampling2D(x):\n",
    "    xr = C.reshape(x, (x.shape[0], x.shape[1], 1, x.shape[2], 1))\n",
    "    xx = C.splice(xr, xr, axis=-1) # axis=-1 refers to the last axis\n",
    "    xy = C.splice(xx, xx, axis=-3) # axis=-3 refers to the middle axis\n",
    "    r = C.reshape(xy, (x.shape[0], x.shape[1] * 2, x.shape[2] * 2))\n",
    "\n",
    "    return r\n",
    "\n",
    "def create_model(input, num_classes):\n",
    "    conv1 = Convolution((3,3), 32, init=glorot_uniform(), activation=relu, pad=True)(input)\n",
    "    conv1 = Convolution((3,3), 32, init=glorot_uniform(), activation=relu, pad=True)(conv1)\n",
    "    pool1 = MaxPooling((2,2), strides=(2,2))(conv1)\n",
    "\n",
    "    conv2 = Convolution((3,3), 64, init=glorot_uniform(), activation=relu, pad=True)(pool1)\n",
    "    conv2 = Convolution((3,3), 64, init=glorot_uniform(), activation=relu, pad=True)(conv2)\n",
    "    pool2 = MaxPooling((2,2), strides=(2,2))(conv2)\n",
    "\n",
    "    conv3 = Convolution((3,3), 128, init=glorot_uniform(), activation=relu, pad=True)(pool2)\n",
    "    conv3 = Convolution((3,3), 128, init=glorot_uniform(), activation=relu, pad=True)(conv3)\n",
    "    pool3 = MaxPooling((2,2), strides=(2,2))(conv3)\n",
    "\n",
    "    conv4 = Convolution((3,3), 256, init=glorot_uniform(), activation=relu, pad=True)(pool3)\n",
    "    conv4 = Convolution((3,3), 256, init=glorot_uniform(), activation=relu, pad=True)(conv4)\n",
    "    pool4 = MaxPooling((2,2), strides=(2,2))(conv4)\n",
    "\n",
    "    conv5 = Convolution((3,3), 512, init=glorot_uniform(), activation=relu, pad=True)(pool4)\n",
    "    conv5 = Convolution((3,3), 512, init=glorot_uniform(), activation=relu, pad=True)(conv5)\n",
    "   \n",
    "    up6 = C.splice(UpSampling2D(conv5), conv4, axis=0)\n",
    "    conv6 = Convolution((3,3), 256, init=glorot_uniform(), activation=relu, pad=True)(up6)\n",
    "    conv6 = Convolution((3,3), 256, init=glorot_uniform(), activation=relu, pad=True)(conv6)\n",
    "\n",
    "    up7 = C.splice(UpSampling2D(conv6), conv3, axis=0)\n",
    "    conv7 = Convolution((3,3), 128, init=glorot_uniform(), activation=relu, pad=True)(up7)\n",
    "    conv7 = Convolution((3,3), 128, init=glorot_uniform(), activation=relu, pad=True)(conv7)\n",
    "\n",
    "    up8 = C.splice(UpSampling2D(conv7), conv2, axis=0)\n",
    "    conv8 = Convolution((3,3), 64, init=glorot_uniform(), activation=relu, pad=True)(up8)\n",
    "    conv8 = Convolution((3,3), 64, init=glorot_uniform(), activation=relu, pad=True)(conv8)\n",
    "\n",
    "    up9 = C.splice(UpSampling2D(conv8), conv1, axis=0)\n",
    "    conv9 = Convolution((3,3), 64, init=glorot_uniform(), activation=relu, pad=True)(up9)\n",
    "    conv9 = Convolution((3,3), 64, init=glorot_uniform(), activation=relu, pad=True)(conv9)\n",
    "\n",
    "    conv10 = Convolution((1,1), num_classes, init=glorot_uniform(), activation=sigmoid, pad=True)(conv9)\n",
    "\n",
    "    return conv10\n",
    "\n",
    "def dice_coefficient(x, y):\n",
    "    # average of per-channel dice coefficient\n",
    "    # global dice coefificnet doesn't work as class with larger region dominates the metrics\n",
    "    # https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "    intersection = C.reduce_sum(x * y, axis=(1,2))\n",
    "\n",
    "    return C.reduce_mean(2.0 * intersection / (C.reduce_sum(x, axis=(1,2)) + C.reduce_sum(y, axis=(1,2)) + 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_minibatch(data_x, data_y, i, minibatch_size):\n",
    "    sx = data_x[i * minibatch_size:(i + 1) * minibatch_size]\n",
    "    sy = data_y[i * minibatch_size:(i + 1) * minibatch_size]\n",
    "\n",
    "    return sx, sy\n",
    "\n",
    "def measure_error(data_x, data_y, x, y, trainer, minibatch_size):\n",
    "    errors = []\n",
    "    for i in range(0, int(len(data_x) / minibatch_size)):\n",
    "        data_sx, data_sy = slice_minibatch(data_x, data_y, i, minibatch_size)\n",
    "\n",
    "        errors.append(trainer.test_minibatch({x: data_sx, y: data_sy}))\n",
    "\n",
    "    return np.mean(errors)\n",
    "\n",
    "def train(images, masks, dst_folder, file_writer, use_existing=False):\n",
    "    shape = input[0].shape\n",
    "    data_size = input.shape[0]\n",
    "\n",
    "    # Split data\n",
    "    test_portion = int(data_size * 0.1)\n",
    "    indices = np.random.permutation(data_size)\n",
    "    test_indices = indices[:test_portion]\n",
    "    training_indices = indices[test_portion:]\n",
    "\n",
    "    test_data = (images[test_indices], masks[test_indices])\n",
    "    training_data = (images[training_indices], masks[training_indices])\n",
    "\n",
    "    # Create model\n",
    "    x = C.input_variable(shape)\n",
    "    y = C.input_variable(masks[0].shape)\n",
    "\n",
    "    z = cntk_unet.create_model(x, masks.shape[1])\n",
    "    dice_coef = cntk_unet.dice_coefficient(z, y)\n",
    "\n",
    "    # Load the saved model if specified\n",
    "    checkpoint_file = \"cntk-unet.dnn\"\n",
    "    if use_existing:\n",
    "        z.load_model(checkpoint_file)\n",
    "\n",
    "    # Prepare model and trainer\n",
    "    lr = learning_rate_schedule(0.00001, UnitType.sample)\n",
    "    momentum = C.learners.momentum_as_time_constant_schedule(0.9)\n",
    "    trainer = C.Trainer(z, (-dice_coef, -dice_coef), C.learners.adam(z.parameters, lr=lr, momentum=momentum))\n",
    "\n",
    "    # Get minibatches of training data and perform model training\n",
    "    minibatch_size = 8\n",
    "    num_epochs = 75\n",
    "\n",
    "    training_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for e in range(0, num_epochs):\n",
    "        for i in range(0, int(len(training_data[0]) / minibatch_size)):\n",
    "            data_x, data_y = slice_minibatch(training_data[0], training_data[1], i, minibatch_size)\n",
    "\n",
    "            trainer.train_minibatch({input: data_x, label: data_y})\n",
    "\n",
    "        # Measure training error\n",
    "        training_error = measure_error(training_data[0], training_data[1], x, y, trainer, minibatch_size)\n",
    "        training_errors.append(training_error)\n",
    "\n",
    "        # Measure test error\n",
    "        test_error = measure_error(test_data[0], test_data[1], x, y, trainer, minibatch_size)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "        string_log = \"epoch #{}: training_error={}, test_error={}\".format(e, training_errors[-1], test_errors[-1])\n",
    "        string_append_and_print(string_log, file_writer)\n",
    "        \n",
    "        string_saved_model = \"\\t\\tmodel saved from data set {0:.2f}\".format(test_result*100 / num_minibatches_to_test)\n",
    "        string_append_and_print(string_saved_model, path_log)\n",
    "        \n",
    "        trainer.save_checkpoint(checkpoint_file)\n",
    "        z.save(dst_folder + \"/model{0:.2f}.model\".format(test_result*100 / num_minibatches_to_test))\n",
    "    return trainer, training_errors, test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reader_text(path, is_training, input_dim, num_label_classes):   \n",
    "    print(path)\n",
    "    labelStream = cntk.io.StreamDef(field='label', shape=(256, 256), is_sparse=False)\n",
    "    featureStream = cntk.io.StreamDef(field='features', shape=(3, 256, 256), is_sparse=False)\n",
    "#     coordinateStream = cntk.io.StreamDef(field='coordinate', shape=2, is_sparse=False)\n",
    "    \n",
    "#     deserailizer = cntk.io.CBFDeserializer(path, cntk.io.StreamDefs(label = labelStream ,feature = featureStream))\n",
    "    deserailizer = cntk.io.CTFDeserializer(path, cntk.io.StreamDefs(label = labelStream ,feature = featureStream))      \n",
    "    return cntk.io.MinibatchSource(deserailizer,\n",
    "       randomize = False, multithreaded_deserializer=False,max_sweeps = cntk.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "def create_reader_binary(path, is_training, input_dim_1, num_label_classes_1):   \n",
    "#     print(path)\n",
    "    labelStream = cntk.io.StreamDef(field='label', shape=num_label_classes_1, is_sparse=False)\n",
    "    featureStream = cntk.io.StreamDef(field='features', shape=input_dim_1, is_sparse=False)\n",
    "#     coordinateStream = cntk.io.StreamDef(field='coordinate', shape=2, is_sparse=False)\n",
    "    \n",
    "    deserailizer = cntk.io.CBFDeserializer(path, cntk.io.StreamDefs(label = labelStream ,feature = featureStream))\n",
    "#     deserailizer = cntk.io.CTFDeserializer(path, cntk.io.StreamDefs(label = labelStream ,feature = featureStream))      \n",
    "    return cntk.io.MinibatchSource(deserailizer,\n",
    "       randomize = False, multithreaded_deserializer=False,max_sweeps = cntk.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_criterion_function(model, labels):\n",
    "    loss = cntk.cross_entropy_with_softmax(model, labels)\n",
    "    errs = cntk.classification_error(model, labels)\n",
    "    return loss, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "#     if mb%frequency == 0:\n",
    "    training_loss = trainer.previous_minibatch_loss_average\n",
    "    eval_error = trainer.previous_minibatch_evaluation_average\n",
    "#         if verbose: \n",
    "    string_timing = \"\\t\\tMinibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100)\n",
    "    string_append_and_print(string_timing, path_log)\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_adaptive_learner(model, base_lr_per_sample, minibatch_size, num_samples_per_sweep):\n",
    "    lr_schedule = get_adaptive_learning_schedule(base_lr_per_sample, minibatch_size, num_samples_per_sweep)\n",
    "    #momentum_time_constant = -minibatch_size / np.log(0.98)\n",
    "    #momentum_time_constant = [3000]\n",
    "    l2_reg_weight = 0.0001  # 0.0001\n",
    "    #mm_schedule = cntk.momentum_as_time_constant_schedule(momentum_time_constant)\n",
    "    mm_schedule = cntk.momentum_schedule(0.90)\n",
    "\n",
    "    learner = cntk.momentum_sgd(model.parameters, lr_schedule, mm_schedule,\n",
    "                           l2_regularization_weight=l2_reg_weight, unit_gain=True)\n",
    "\n",
    "    return learner\n",
    "    \n",
    "def get_adaptive_learning_schedule(base_lr_per_sample, minibatch_size, num_samples_per_sweep):\n",
    "    return cntk.learners.learning_parameter_schedule(base_lr_per_sample, minibatch_size, num_samples_per_sweep)\n",
    "\n",
    "def calculate_learning_rate(num_samples_per_sweep, minibatch_size ,num_sweeps_to_train_with) :\n",
    "    model = z(input/255)\n",
    "    loss, label_error = create_criterion_function(model, label)\n",
    "    \n",
    "    k = int(num_samples_per_sweep / minibatch_size)\n",
    "    num_k = num_sweeps_to_train_with // 5\n",
    "    learning_rate = ([0.2]*(k*num_k))+([0.1]*(k*num_k))+([0.05]*(k*num_k))+([0.025]*(k*num_k))+([0.0125]*(k*num_k))\n",
    "#     learning_rate = [0.2] * k\n",
    "    lr_schedule = cntk.learning_rate_schedule(learning_rate, cntk.UnitType.minibatch)\n",
    "    learner =  set_adaptive_learner(z, learning_rate, minibatch_size, num_samples_per_sweep)\n",
    "#     learner = cntk.sgd(z.parameters, lr_schedule)\n",
    "    trainer = cntk.Trainer(z, (loss, label_error), [learner])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_append_and_print(line, file) :\n",
    "    file_writer = open(file, 'a')\n",
    "    file_writer.write(line + \"\\n\")\n",
    "    file_writer.close()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(plotdata):\n",
    "    plt.plot(plotdata[\"mb\"], plotdata[\"loss\"], '--r')\n",
    "    plt.xlabel('Minibatch number')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_model = (3, 256, 256)\n",
    "input_dim = 3 * 256 * 256\n",
    "num_output_classes = 256*256\n",
    "input = cntk.input_variable(input_dim_model)  # สังเกตว่าเราใช้ input_dim_model เป็นพารามิเตอร์แทนการใช้ input_dim\n",
    "label = cntk.input_variable((256, 256))\n",
    "\n",
    "samples_test_size = len(open(\"../training_data/features/original/samples_test.txt\", \"r\").readlines())\n",
    "samples_train_size = len(open(\"../training_data/features/original/samples_train.txt\", \"r\").readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def train_test(train_reader, test_reader, model_func , path_log, num_sweeps_to_train_with, num_samples_train, num_samples_test, dst) :\n",
    "    \n",
    "    minibatch_size = 64\n",
    "    num_samples_per_sweep = num_samples_train\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "    \n",
    "    trainer = calculate_learning_rate(num_samples_per_sweep, minibatch_size ,num_sweeps_to_train_with)\n",
    "    \n",
    "    input_map={\n",
    "        label  : train_reader.streams.label,\n",
    "        input  : train_reader.streams.feature\n",
    "    } \n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    training_progress_output_freq = 0\n",
    "     \n",
    "    # Start a timer\n",
    "    start = time.time()\n",
    "    l_list = []\n",
    "    error = []\n",
    "    loss_per_sweep = []\n",
    "    for i in range(0, int(num_minibatches_to_train)):\n",
    "        # Read a mini batch from the training data file\n",
    "        data=train_reader.next_minibatch(minibatch_size, input_map=input_map) \n",
    "        training_data = []\n",
    "        for k in data.keys() :\n",
    "            for v in data[k].as_sequences()[0] :\n",
    "                 training_data.append(v)\n",
    "        \n",
    "\n",
    "        return data_x, data_y \n",
    "\n",
    "        trainer.train_minibatch({input: data_y, label: data_x})\n",
    "\n",
    "#         a, l, e = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "#         a, l, e = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "        if i % training_progress_output_freq == 0 :\n",
    "#             print(i)\n",
    "            a, l, e = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "            l_list.append(l)\n",
    "            error.append(e)\n",
    "        if i % k == 0 :\n",
    "            \n",
    "            a, l, e = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "            string_training_time = \"\\t\\ttraining took per sweep round = {:.0f} => {:.1f} sec\".format((i+1)/k,time.time() - start)\n",
    "            string_append_and_print(string_training_time, path_log)\n",
    "            plotdata[\"mb\"].append(i)\n",
    "            plotdata[\"loss\"].append(l)\n",
    "            loss_per_sweep.append(l)\n",
    "#             print(\"loss per sweep to train = \" + str(l))\n",
    "       \n",
    "     \n",
    "    # Print training time\n",
    "    string_training_time = \"\\t\\tTraining took {:.1f} sec\".format(time.time() - start)\n",
    "    string_append_and_print(string_training_time, path_log)\n",
    "    \n",
    "    # Test the model\n",
    "    test_input_map = {\n",
    "        label  : test_reader.streams.label,\n",
    "        input  : test_reader.streams.feature\n",
    "    }\n",
    "\n",
    "    # Test data for trained model\n",
    "    test_minibatch_size = 64\n",
    "    \n",
    "#     num_samples = len( open(name,'r').readlines() )\n",
    "    num_samples = num_samples_test\n",
    "    num_minibatches_to_test = num_samples_test // test_minibatch_size\n",
    "    \n",
    "    test_result = 0.0   \n",
    "\n",
    "    for i in range(num_minibatches_to_test):\n",
    "    \n",
    "        # We are loading test data in batches specified by test_minibatch_size\n",
    "        # Each data point in the minibatch is a MNIST digit image of 784 dimensions \n",
    "        # with one pixel per dimension that we will encode / decode with the \n",
    "        # trained model.\n",
    "        data = test_reader.next_minibatch(test_minibatch_size, input_map=test_input_map)\n",
    "        \n",
    "       \n",
    "        # reshpae data this here.\n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "        test_result = test_result + eval_error\n",
    "    \n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    string_average_test_error = \"\\t\\tAverage test error: {0:.2f}%\".format(test_result*100 / num_minibatches_to_test)\n",
    "    string_append_and_print(string_average_test_error, path_log)\n",
    "  \n",
    "    #save model whem error late less number target\n",
    "    path_model = dst + \"/model{0:.2f}.model\".format(test_result*100 / num_minibatches_to_test)\n",
    "#     if((test_result*100 / num_minibatches_to_test) < 9) :\n",
    "    string_saved_model = \"\\t\\tmodel saved from data set {0:.2f}\".format(test_result*100 / num_minibatches_to_test)\n",
    "    string_append_and_print(string_saved_model, path_log)\n",
    "    model_list.append(z)\n",
    "#     if (test_result*100 / num_minibatches_to_test) <= 8.00 :\n",
    "    z.save(path_model)\n",
    "    return \"{0:.2f}\".format(test_result*100 / num_minibatches_to_test), l_list, loss_per_sweep,error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train_test(reader_train, reader_test, input_dim, path_log ,num_output_classes, num_sweeps_to_train_with, num_samples_train, num_samples_test, dst):\n",
    "    global z\n",
    "    z = create_model(input, (256,256))\n",
    "#     return train(reader_train, reader_test, dst, path_log)\n",
    "    return train_test(reader_train, reader_test, z, path_log, num_sweeps_to_train_with, num_samples_train, num_samples_test,dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_test(src) :\n",
    "    src_test_file = src + \"samples_test.dat\"\n",
    "    num_samples = samples_test_size\n",
    "    \n",
    "    return create_reader_binary(src_test_file, False, input_dim, num_output_classes), num_samples\n",
    "\n",
    "def load_train(src) :\n",
    "    src_train_file = src + \"samples_train.dat\"\n",
    "    num_samples = samples_train_size\n",
    "        \n",
    "    return create_reader_binary(src_train_file, True, input_dim, num_output_classes), num_samples\n",
    "    \n",
    "def load_config(src) :\n",
    "    train_per_sweep = 10\n",
    "    reader_train, num_samples_train = load_train(src)\n",
    "    reader_test, num_samples_test = load_test(src)\n",
    "    \n",
    "    return train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder_exist(src_folder) :\n",
    "    return os.path.exists(src_folder)\n",
    "\n",
    "def create_folder(src_folder) :\n",
    "    folders = src_folder.split(\"/\")\n",
    "    current_folder = \"\"\n",
    "    \n",
    "    for folder in folders :\n",
    "        current_folder += folder + \"/\"\n",
    "        if not (folder == '' or folder == '.' or folder == '..') and not (check_folder_exist(current_folder)):\n",
    "            os.mkdir(current_folder)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def process(src_folder, dst_folder) :\n",
    "    \n",
    "    create_folder(dst_folder)\n",
    "    time = datetime.now(timezone('Asia/Bangkok')).strftime('log_date_%Y-%m-%d_time_%H-%M-%S')\n",
    "    \n",
    "    path_log = dst_folder + time + \".txt\"\n",
    "    file_writer = open(path_log, 'w')\n",
    "    \n",
    "    train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test = load_config(src_folder)\n",
    "    string_filter_details = \"folder => {:s}, train_per_sweep => {:d}\\n\".format(src_folder, train_per_sweep)\n",
    "    \n",
    "    \n",
    "    return do_train_test(reader_train, reader_test, input_dim, path_log, num_output_classes, train_per_sweep, num_samples_train, num_samples_test, dst_folder)\n",
    "#     return reader_train, reader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-19dddc726ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msrc_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../training_data/features/original/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdst_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../training_data/model/original/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# t, t1, t2 = process(src_folder, dst_folder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-251-156bec469718>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(src_folder, dst_folder)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdo_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_per_sweep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#     return reader_train, reader_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-248-53b0daba72d6>\u001b[0m in \u001b[0;36mdo_train_test\u001b[0;34m(reader_train, reader_test, input_dim, path_log, num_output_classes, num_sweeps_to_train_with, num_samples_train, num_samples_test, dst)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     return train(reader_train, reader_test, dst, path_log)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sweeps_to_train_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-247-cb69ca71616c>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(train_reader, test_reader, model_func, path_log, num_sweeps_to_train_with, num_samples_train, num_samples_test, dst)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata_x\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_x' is not defined"
     ]
    }
   ],
   "source": [
    "src_folder = \"../training_data/features/original/\"\n",
    "dst_folder = \"../training_data/model/original/\"\n",
    "x,y= process(src_folder, dst_folder)\n",
    "# t, t1, t2 = process(src_folder, dst_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The input (../training_data/features/original/samples_train.txt) is not a valid CNTK binary format file.\n\n[CALL STACK]\n[0x7f592be2486c]                                                       + 0x1186c\n[0x7f592be26a73]    CNTK::BinaryChunkDeserializer::  Initialize  (std::map<std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>>,std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>>,std::less<std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>>>,std::allocator<std::pair<std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>> const,std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>>>>> const&,  CNTK::DataType) + 0x9a3\n[0x7f592be26bcb]    CNTK::BinaryChunkDeserializer::  BinaryChunkDeserializer  (CNTK::BinaryConfigHelper const&) + 0x2b\n[0x7f592be23fb7]    CreateDeserializer                                 + 0x97\n[0x7f592c053d67]    CNTK::CompositeDataReader::  CreateDeserializer  (Microsoft::MSR::CNTK::ConfigParameters const&,  bool) + 0x1d7\n[0x7f592c054161]    CNTK::CompositeDataReader::  CreateDeserializers  (Microsoft::MSR::CNTK::ConfigParameters const&) + 0x281\n[0x7f592c054b1a]    CNTK::CompositeDataReader::  CompositeDataReader  (Microsoft::MSR::CNTK::ConfigParameters const&) + 0x75a\n[0x7f592c05eb06]    CreateCompositeDataReader                          + 0x26\n[0x7f596c9eaaa0]    CNTK::CompositeMinibatchSource::  CompositeMinibatchSource  (CNTK::MinibatchSourceConfig const&) + 0x5f0\n[0x7f596c9eaee7]    CNTK::  CreateCompositeMinibatchSource  (CNTK::MinibatchSourceConfig const&) + 0x27\n[0x7f596d527fd9]                                                       + 0x11ffd9\n[0x7f5988b3a429]    _PyCFunction_FastCallDict                          + 0x229\n[0x7f5988bbfb8c]                                                       + 0x148b8c\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe39c]    _PyFunction_FastCallDict                           + 0x10c\n[0x7f5988ae2ce6]    _PyObject_FastCallDict                             + 0x166\n[0x7f5988ae2f3c]    _PyObject_Call_Prepend                             + 0xcc\n[0x7f5988ae2fd6]    PyObject_Call                                      + 0x56\n[0x7f5988b5bad3]                                                       + 0xe4ad3\n[0x7f5988b512ac]                                                       + 0xda2ac\n[0x7f5988ae2c1e]    _PyObject_FastCallDict                             + 0x9e\n[0x7f5988ae2dd8]    _PyObject_FastCallKeywords                         + 0xa8\n[0x7f5988bbf95b]                                                       + 0x14895b\n[0x7f5988bc32cc]    _PyEval_EvalFrameDefault                           + 0x31cc\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe583]    PyEval_EvalCodeEx                                  + 0x63\n[0x7f5988bbe5cb]    PyEval_EvalCode                                    + 0x3b\n[0x7f5988bbbf6e]                                                       + 0x144f6e\n[0x7f5988b3a429]    _PyCFunction_FastCallDict                          + 0x229\n[0x7f5988bbfb8c]                                                       + 0x148b8c\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc32cc]    _PyEval_EvalFrameDefault                           + 0x31cc\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe39c]    _PyFunction_FastCallDict                           + 0x10c\n[0x7f5988ae2ce6]    _PyObject_FastCallDict                             + 0x166\n[0x7f5988ae2f3c]    _PyObject_Call_Prepend                             + 0xcc\n[0x7f5988ae2fd6]    PyObject_Call                                      + 0x56\n[0x7f5988bc3fc9]    _PyEval_EvalFrameDefault                           + 0x3ec9\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc32cc]    _PyEval_EvalFrameDefault                           + 0x31cc\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe583]    PyEval_EvalCodeEx                                  + 0x63\n[0x7f5988b161a1]                                                       + 0x9f1a1\n[0x7f5988ae2fd6]    PyObject_Call                                      + 0x56\n[0x7f5988bc3fc9]    _PyEval_EvalFrameDefault                           + 0x3ec9\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe583]    PyEval_EvalCodeEx                                  + 0x63\n[0x7f5988b161a1]                                                       + 0x9f1a1\n[0x7f5988ae2fd6]    PyObject_Call                                      + 0x56\n[0x7f5988bc3fc9]    _PyEval_EvalFrameDefault                           + 0x3ec9\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbe515]    _PyFunction_FastCallDict                           + 0x285\n[0x7f5988ae2ce6]    _PyObject_FastCallDict                             + 0x166\n[0x7f5988ae2f3c]    _PyObject_Call_Prepend                             + 0xcc\n[0x7f5988ae2fd6]    PyObject_Call                                      + 0x56\n[0x7f5988bc3fc9]    _PyEval_EvalFrameDefault                           + 0x3ec9\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe583]    PyEval_EvalCodeEx                                  + 0x63\n[0x7f5988bbe5cb]    PyEval_EvalCode                                    + 0x3b\n[0x7f5988bbbf6e]                                                       + 0x144f6e\n[0x7f5988b3a429]    _PyCFunction_FastCallDict                          + 0x229\n[0x7f5988bbfb8c]                                                       + 0x148b8c\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe583]    PyEval_EvalCodeEx                                  + 0x63\n[0x7f5988b16082]                                                       + 0x9f082\n[0x7f5988ae2fd6]    PyObject_Call                                      + 0x56\n[0x7f5988c0c732]                                                       + 0x195732\n[0x7f5988c0d4cd]    Py_Main                                            + 0xa1d\n[0x400c1d]          main                                               + 0x16d\n[0x7f5987b98830]    __libc_start_main                                  + 0xf0\n[0x4009e9]                                                            \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-33aebddf9531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mminibatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m46\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_samples_per_sweep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples_train_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_minibatches_to_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_samples_per_sweep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-181-156bec469718>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(src_folder, dst_folder)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfile_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_per_sweep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mstring_filter_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"folder => {:s}, train_per_sweep => {:d}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_per_sweep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-184-82a58abe1030>\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(src)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_per_sweep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mreader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mreader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-184-82a58abe1030>\u001b[0m in \u001b[0;36mload_train\u001b[0;34m(src)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples_train_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_reader_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_train_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-176-bc404bbd33a2>\u001b[0m in \u001b[0;36mcreate_reader_binary\u001b[0;34m(path, is_training, input_dim_1, num_label_classes_1)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#     deserailizer = cntk.io.CTFDeserializer(path, cntk.io.StreamDefs(label = labelStream ,feature = featureStream))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     return cntk.io.MinibatchSource(deserailizer,\n\u001b[0;32m---> 21\u001b[0;31m        randomize = False, multithreaded_deserializer=False,max_sweeps = cntk.io.INFINITELY_REPEAT if is_training else 1)\n\u001b[0m",
      "\u001b[0;32m/home/anaconda3/envs/cntk36/lib/python3.6/site-packages/cntk/io/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, deserializers, max_samples, max_sweeps, randomization_window_in_chunks, randomization_window_in_samples, randomization_seed, trace_level, multithreaded_deserializer, frame_mode, truncation_length, randomize)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomization_window_in_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_composite_minibatch_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;31m# transplant into this class instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The input (../training_data/features/original/samples_train.txt) is not a valid CNTK binary format file.\n\n[CALL STACK]\n[0x7f592be2486c]                                                       + 0x1186c\n[0x7f592be26a73]    CNTK::BinaryChunkDeserializer::  Initialize  (std::map<std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>>,std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>>,std::less<std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>>>,std::allocator<std::pair<std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>> const,std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>>>>> const&,  CNTK::DataType) + 0x9a3\n[0x7f592be26bcb]    CNTK::BinaryChunkDeserializer::  BinaryChunkDeserializer  (CNTK::BinaryConfigHelper const&) + 0x2b\n[0x7f592be23fb7]    CreateDeserializer                                 + 0x97\n[0x7f592c053d67]    CNTK::CompositeDataReader::  CreateDeserializer  (Microsoft::MSR::CNTK::ConfigParameters const&,  bool) + 0x1d7\n[0x7f592c054161]    CNTK::CompositeDataReader::  CreateDeserializers  (Microsoft::MSR::CNTK::ConfigParameters const&) + 0x281\n[0x7f592c054b1a]    CNTK::CompositeDataReader::  CompositeDataReader  (Microsoft::MSR::CNTK::ConfigParameters const&) + 0x75a\n[0x7f592c05eb06]    CreateCompositeDataReader                          + 0x26\n[0x7f596c9eaaa0]    CNTK::CompositeMinibatchSource::  CompositeMinibatchSource  (CNTK::MinibatchSourceConfig const&) + 0x5f0\n[0x7f596c9eaee7]    CNTK::  CreateCompositeMinibatchSource  (CNTK::MinibatchSourceConfig const&) + 0x27\n[0x7f596d527fd9]                                                       + 0x11ffd9\n[0x7f5988b3a429]    _PyCFunction_FastCallDict                          + 0x229\n[0x7f5988bbfb8c]                                                       + 0x148b8c\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe39c]    _PyFunction_FastCallDict                           + 0x10c\n[0x7f5988ae2ce6]    _PyObject_FastCallDict                             + 0x166\n[0x7f5988ae2f3c]    _PyObject_Call_Prepend                             + 0xcc\n[0x7f5988ae2fd6]    PyObject_Call                                      + 0x56\n[0x7f5988b5bad3]                                                       + 0xe4ad3\n[0x7f5988b512ac]                                                       + 0xda2ac\n[0x7f5988ae2c1e]    _PyObject_FastCallDict                             + 0x9e\n[0x7f5988ae2dd8]    _PyObject_FastCallKeywords                         + 0xa8\n[0x7f5988bbf95b]                                                       + 0x14895b\n[0x7f5988bc32cc]    _PyEval_EvalFrameDefault                           + 0x31cc\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe583]    PyEval_EvalCodeEx                                  + 0x63\n[0x7f5988bbe5cb]    PyEval_EvalCode                                    + 0x3b\n[0x7f5988bbbf6e]                                                       + 0x144f6e\n[0x7f5988b3a429]    _PyCFunction_FastCallDict                          + 0x229\n[0x7f5988bbfb8c]                                                       + 0x148b8c\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc32cc]    _PyEval_EvalFrameDefault                           + 0x31cc\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe39c]    _PyFunction_FastCallDict                           + 0x10c\n[0x7f5988ae2ce6]    _PyObject_FastCallDict                             + 0x166\n[0x7f5988ae2f3c]    _PyObject_Call_Prepend                             + 0xcc\n[0x7f5988ae2fd6]    PyObject_Call                                      + 0x56\n[0x7f5988bc3fc9]    _PyEval_EvalFrameDefault                           + 0x3ec9\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc32cc]    _PyEval_EvalFrameDefault                           + 0x31cc\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe583]    PyEval_EvalCodeEx                                  + 0x63\n[0x7f5988b161a1]                                                       + 0x9f1a1\n[0x7f5988ae2fd6]    PyObject_Call                                      + 0x56\n[0x7f5988bc3fc9]    _PyEval_EvalFrameDefault                           + 0x3ec9\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe583]    PyEval_EvalCodeEx                                  + 0x63\n[0x7f5988b161a1]                                                       + 0x9f1a1\n[0x7f5988ae2fd6]    PyObject_Call                                      + 0x56\n[0x7f5988bc3fc9]    _PyEval_EvalFrameDefault                           + 0x3ec9\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbe515]    _PyFunction_FastCallDict                           + 0x285\n[0x7f5988ae2ce6]    _PyObject_FastCallDict                             + 0x166\n[0x7f5988ae2f3c]    _PyObject_Call_Prepend                             + 0xcc\n[0x7f5988ae2fd6]    PyObject_Call                                      + 0x56\n[0x7f5988bc3fc9]    _PyEval_EvalFrameDefault                           + 0x3ec9\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbd514]                                                       + 0x146514\n[0x7f5988bbfc88]                                                       + 0x148c88\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe583]    PyEval_EvalCodeEx                                  + 0x63\n[0x7f5988bbe5cb]    PyEval_EvalCode                                    + 0x3b\n[0x7f5988bbbf6e]                                                       + 0x144f6e\n[0x7f5988b3a429]    _PyCFunction_FastCallDict                          + 0x229\n[0x7f5988bbfb8c]                                                       + 0x148b8c\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbfb2a]                                                       + 0x148b2a\n[0x7f5988bc2d40]    _PyEval_EvalFrameDefault                           + 0x2c40\n[0x7f5988bbe100]                                                       + 0x147100\n[0x7f5988bbe583]    PyEval_EvalCodeEx                                  + 0x63\n[0x7f5988b16082]                                                       + 0x9f082\n[0x7f5988ae2fd6]    PyObject_Call                                      + 0x56\n[0x7f5988c0c732]                                                       + 0x195732\n[0x7f5988c0d4cd]    Py_Main                                            + 0xa1d\n[0x400c1d]          main                                               + 0x16d\n[0x7f5987b98830]    __libc_start_main                                  + 0xf0\n[0x4009e9]                                                            \n"
     ]
    }
   ],
   "source": [
    "reader_train, reader_test = process(src_folder, dst_folder)\n",
    "minibatch_size = 46\n",
    "num_samples_per_sweep = samples_train_size\n",
    "num_minibatches_to_train = (num_samples_per_sweep * 10) / minibatch_size\n",
    "\n",
    "input_map={\n",
    "        label  : reader_train.streams.label,\n",
    "        input  : reader_train.streams.feature\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[feature([196608]), label([65536])]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_train.stream_infos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-f24a7fee8875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreader_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "data = reader_train.next_minibatch(minibatch_size, input_map=input_map) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Input('Input36107', [#], [256 x 256]): MinibatchData(data=Value([46 x 1 x 65536], GPU), samples=46, seqs=46),\n",
       " Input('Input36106', [#], [3 x 256 x 256]): MinibatchData(data=Value([46 x 1 x 196608], GPU), samples=46, seqs=46)}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  0. ...,  0.  0.  0.]\n",
      "[  94.   96.   98. ...,  255.  255.  255.]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k in data.keys() :\n",
    "    for v in data[k].as_sequences()[0] :\n",
    "         print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation import segmentation\n",
    "loss = []\n",
    "loss_per_sweep = []\n",
    "error = []\n",
    "\n",
    "def process1(tissue, src_testing_data):\n",
    "    s = datetime.now(timezone('Asia/Bangkok')).strftime('log_date_%Y-%m-%d_time_%H-%M-%S')\n",
    "    \n",
    "    error_rates = []\n",
    "    sum_error_rate = 0\n",
    "    \n",
    "    src_root = \"../training_data/features/\"\n",
    "    src = src_root + tissue + \"/\"\n",
    "    \n",
    "    dst_root = \"../training_data/model/\"\n",
    "    dst = dst_root + tissue + \"/\"\n",
    "    \n",
    "    create_dir(dst_root)\n",
    "    create_dir(dst)\n",
    "    \n",
    "    path_log = dst + s  + \".txt\"\n",
    "    file_writer = open(path_log, 'w')\n",
    "    \n",
    "    for index in range(1,2):\n",
    "        src_k =  src + \"k-\" + str(index+1) + \"/\"\n",
    "        filter, stride, filters,  train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test = load_config(src_k)\n",
    "        \n",
    "        \n",
    "        string_filter_details = \"folder => {:s}, train_per_sweep => {:d}\\nfilter => {:d}, stride => {:d}, dept => {:d}, {:d} {:d} {:d}\".format(src_k, train_per_sweep, filter, stride, len(filters), filters[0], filters[1], filters[2])\n",
    "        string_append_and_print(string_filter_details, path_log)\n",
    "        \n",
    "        error_rate, l, ls,e = do_train_test(reader_train, reader_test, input_dim, index+1, path_log, num_output_classes, filter, stride, filters, train_per_sweep, num_samples_train, num_samples_test, dst)\n",
    "        \n",
    "        segmentation().segment1(dst, \"model\"+str(error_rate)+\".model\", src_testing_data)\n",
    "    \n",
    "# process1(\"granulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plotdata = defaultdict(list)\n",
    "src_testing_data = '../testing_data/granulation/'\n",
    "process1(\"/granulation\", src_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotdata = defaultdict(list)\n",
    "src_testing_data = '../testing_data/necrotic/'\n",
    "process1(\"necrotic\", src_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotdata = defaultdict(list)\n",
    "src_testing_data = '../testing_data/slough/'\n",
    "process1(\"/slough\", src_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotdata = defaultdict(list)\n",
    "# src_testing_data = '../testing_data/original/'\n",
    "# process1(\"/original\", src_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plotdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process1(\"granulation\")\n",
    "# plot_1000 = plotdata\n",
    "plot(plot_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process1(\"augmen_motoC_to_s8_and_s8_to_iPad/necrotic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process1(\"augmen_motoC_to_s8_and_s8_to_iPad/slough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process1(\"augmen_motoC_to_s8_and_s8_to_iPad/original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "loss_per_sweep = []\n",
    "error = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# def process1(filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test) :\n",
    "#     error_rates = []\n",
    "#     sum_error_rate = 0\n",
    "#     file_writer = open(path_log, 'a')\n",
    "    \n",
    "#     string_filter_details = \"filter => {:d}, stride => {:d}, dept => {:d}, {:d} {:d} {:d}\".format(filter, stride, len(filters), filters[0], filters[1], filters[2])\n",
    "#     string_append_and_print(string_filter_details, path_log)\n",
    "\n",
    "#     file_writer.close()\n",
    "#     file_writer = open(path_log, 'a')\n",
    "    \n",
    "#     #k-flod validation = 5\n",
    "    \n",
    "#     src_root = \"../training_data/features/\"\n",
    "#     src_granulation = src_root + \"granulation/\"\n",
    "    \n",
    "#     dst_root = \"../training_data/model/\"\n",
    "#     dst_granulation = dst_root + \"granulation/\"\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for index in range(5):\n",
    "#         #address file train and test\n",
    "#         src_granulation_k = \"k-\" + str(index+1)\n",
    "#         string_index_data_set = '\\tData Set : ' + str(i)\n",
    "#         string_append_and_print(string_index_data_set, path_log)\n",
    "        \n",
    "#         #read file train and test\n",
    "# #         train_file = os.path.join(data_dir, \"train.dat\")\n",
    "# #         test_file = os.path.join(data_dir, \"test.dat\")\n",
    "        \n",
    "#         #start train and test\n",
    "#         error_rate, l, ls,e = do_train_test(reader_train, reader_test, input_dim, i, file_writer, num_output_classes, filter, stride, filters, train_per_sweep, num_samples_train, num_samples_test)\n",
    "#         sum_error_rate += error_rate\n",
    "#         loss.append(l)\n",
    "#         loss_per_sweep.append(ls)\n",
    "#         error.append(e)\n",
    "#         #append error rate to list for calculate average error late\n",
    "#         error_rates.append(error_rate)\n",
    "\n",
    "#         if index_model > 1 :\n",
    "#             break\n",
    "\n",
    "#     string_file_end_line = \"\\tAverage model error: {0:.2f}%\\n\\tmin model error: {0:.2f}%\\n\\t>>>>>>>>>>>>>>>>>>\\n>>>>>>>>>>>>>>>>>>\".format(sum_error_rate/train_per_sweep,min(error_rates))     \n",
    "#     string_append_and_print(string_file_end_line, path_log)\n",
    "    \n",
    "#     file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_180_test_less_filter_per_10_sweep()\n",
    "# filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_180_test_less_filter_per_50_sweep()\n",
    "# filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_180_train_less_filter_per_10_sweep()\n",
    "# filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_180_train_more_filter_per_10_sweep()\n",
    "# filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_180_train_less_filter_per_50_sweep()\n",
    "# filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_14400_test_less_filter_per_10_sweep()\n",
    "filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_14400_train_less_filter_per_10_sweep()\n",
    "open(path_log,'w')\n",
    "print(num_samples_train)\n",
    "print(num_samples_test)\n",
    "# process1(5, 2, [256, 512, 1024],  50, path_log)\n",
    "process1(filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "# from cntk.ops.functions import load_model\n",
    "# current_model = load_model(\"output/model/model4.model\")\n",
    "# print(z)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(loss_per_sweep[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = list(range(0,len(loss_per_sweep[0])*60000,60000))\n",
    "plt.plot(mb, loss_per_sweep[0])\n",
    "plt.show()\n",
    "# plt.figure(1)\n",
    "# for l in range(len(loss_per_sweep)-1) :\n",
    "#     sub_plot = \"33\" + str((int(l) + 1))\n",
    "#     plt.subplot(int(sub_plot))\n",
    "#     plt.plot(mb, loss_per_sweep[l])\n",
    "#     plt.title(\"data set \" + str((int(l) + 1)))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = list(range(0,len(loss[0])*10000,10000))\n",
    "plt.figure(1)\n",
    "for l in range(len(loss)-1) :\n",
    "    sub_plot = \"33\" + str((int(l) + 1))\n",
    "    plt.subplot(int(sub_plot))\n",
    "    plt.plot(mb, loss[l])\n",
    "    plt.title(\"data set \" + str((int(l) + 1)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import TestingData\n",
    "testing_data, labels, size_file = TestingData().load(\"../testing_data/input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_label = []\n",
    "\n",
    "for i in range(0, size_file):\n",
    "    prediction_label.append(z.eval(testing_data[i])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_label(index) :\n",
    "    if index == 0 :\n",
    "        return \"1 0 0\"\n",
    "    elif index == 1 :\n",
    "        return \"0 1 0\"\n",
    "    else :\n",
    "        return \"0 0 1\"\n",
    "\n",
    "prediction_labels = []\n",
    "# path_file_prediction = \"output_label/prediction_label.txt\"\n",
    "# file = open(path_file_prediction, 'w')\n",
    "# file = open(path_file_prediction, 'a')\n",
    "\n",
    "for current_arr in prediction_label :\n",
    "# #     file.write(predict_label(np.argmax(current_arr))+\"\\n\")\n",
    "    prediction_labels.append(predict_label(np.argmax(current_arr)))\n",
    "# file.close()\n",
    "\n",
    "size_labels = len(labels)\n",
    "t_bounary, t_inner, t_outer = 0, 0, 0\n",
    "n_bounary_inner, n_bounary_outer = 0, 0\n",
    "n_inner_bounary, n_inner_outer = 0, 0\n",
    "n_outer_bounary, n_outer_inner = 0, 0,\n",
    "for i in range(0, size_labels) :\n",
    "    if labels[i] == \"0 0 1\":\n",
    "        if labels[i] == prediction_labels[i]:\n",
    "            t_bounary += 1 \n",
    "        elif prediction_labels[i] == \"0 1 0\" :\n",
    "            n_bounary_inner += 1\n",
    "        elif prediction_labels[i] == \"1 0 0\" :\n",
    "            n_bounary_outer += 1\n",
    "    elif labels[i] == \"0 1 0\":\n",
    "        if labels[i] == prediction_labels[i]:\n",
    "            t_inner += 1 \n",
    "        elif prediction_labels[i] == \"0 0 1\" :\n",
    "            n_inner_bounary += 1\n",
    "        elif prediction_labels[i] == \"1 0 0\" :\n",
    "            n_inner_outer += 1\n",
    "    elif labels[i] == \"1 0 0\":\n",
    "        if labels[i] == prediction_labels[i]:\n",
    "            t_outer += 1 \n",
    "        elif prediction_labels[i] == \"0 0 1\" :\n",
    "            n_outer_bounary += 1\n",
    "        elif prediction_labels[i] == \"0 1 0\" :\n",
    "            n_outer_inner += 1\n",
    "print(str(t_bounary) + \" \" + str(n_bounary_inner) + \" \" + str(n_bounary_outer))\n",
    "print(str(n_inner_bounary) + \" \" + str(t_inner) + \" \" + str(n_inner_outer))\n",
    "print(str(n_outer_bounary) + \" \" + str( n_outer_inner) + \" \" + str(n_bounary_outer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform testing data for prediction\n",
    "\n",
    "# def number_of_line(path):\n",
    "#     file = open(path, 'r')\n",
    "#     f = []\n",
    "#     while True :\n",
    "#         line = file.readline();\n",
    "#         if not line :\n",
    "#             break\n",
    "#         f.append(line)\n",
    "#     return len(f)\n",
    "\n",
    "# def prediction_file(path_directory):\n",
    "#     arr = []\n",
    "    \n",
    "#     path_file = \"testing_features_data.txt\"\n",
    "#     size_file = number_of_line(path_directory + path_file)\n",
    "\n",
    "#     testing_file = os.path.join(path_directory, path_file)\n",
    "#     reader_test = create_reader(testing_file, False, input_dim, num_output_classes)\n",
    "#     test_input_map = {\n",
    "#             input  : reader_test.streams.features,\n",
    "#             label : reader_test.streams.labels\n",
    "#         }\n",
    "#     data = reader_test.next_minibatch(size_file, input_map=test_input_map)\n",
    "#     data_asarray =  data[input].asarray()\n",
    "\n",
    "#     patch = np.reshape(data_asarray[0], (3,31,31))\n",
    "#     print(z.eval(patch)[0])\n",
    "# #     z.eval(data_asarray)\n",
    "# #     for i in range(0, size_file):\n",
    "# #         patch = np.reshape(data_asarray[i], (3,31,31))\n",
    "# #         print(current_model.eval(patch)[0])\n",
    "# #         arr.append(current_model.eval(patch)[0])\n",
    "# #     print(len(arr))\n",
    "#     return arr\n",
    "\n",
    "# arr = prediction_file(\"../testing_data/input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #prediction and write answer in file \n",
    "\n",
    "# predict_labels = []\n",
    "\n",
    "# #prediction method => index : position label max\n",
    "# def predict_label(index) :\n",
    "#     if index == 0 :\n",
    "#         return \"1 0 0\"\n",
    "#     elif index == 1 :\n",
    "#         return \"0 1 0\"\n",
    "#     else :\n",
    "#         return \"0 0 1\"\n",
    "\n",
    "# path_file_prediction = \"output_label/prediction_label.txt\"\n",
    "# file = open(path_file_prediction, 'w')\n",
    "# file = open(path_file_prediction, 'a')\n",
    "\n",
    "# for current_arr in arr :\n",
    "#     file.write(predict_label(np.argmax(current_arr))+\"\\n\")\n",
    "#     predict_labels.append(predict_label(np.argmax(current_arr)))\n",
    "# file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test set data\n",
    "# labels = []\n",
    "# def get_label_from_testing_data(path):    \n",
    "#     file_test = open(path)\n",
    "#     while True :\n",
    "#         line = file_test.readline()\n",
    "#         if not line :\n",
    "#             break\n",
    "#         lines = line.split(\" \")\n",
    "#         current_label = lines[1] + \" \" + lines[2] + \" \" + lines[3]\n",
    "#         labels.append(current_label)\n",
    "\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t1.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t2.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t4.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t5.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t6.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t7.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t8.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t9.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t10.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_labels = len(labels)\n",
    "# t_bounary, t_inner, t_outer = 0, 0, 0\n",
    "# n_bounary_inner, n_bounary_outer = 0, 0\n",
    "# n_inner_bounary, n_inner_outer = 0, 0\n",
    "# n_outer_bounary, n_outer_inner = 0, 0,\n",
    "# for i in range(0, size_labels) :\n",
    "#     if labels[i] == \"0 0 1\":\n",
    "#         if labels[i] == predict_labels[i]:\n",
    "#             t_bounary += 1 \n",
    "#         elif predict_labels[i] == \"0 1 0\" :\n",
    "#             n_bounary_inner += 1\n",
    "#         elif predict_labels[i] == \"1 0 0\" :\n",
    "#             n_bounary_outer += 1\n",
    "#     elif labels[i] == \"0 1 0\":\n",
    "#         if labels[i] == predict_labels[i]:\n",
    "#             t_inner += 1 \n",
    "#         elif predict_labels[i] == \"0 0 1\" :\n",
    "#             n_inner_bounary += 1\n",
    "#         elif predict_labels[i] == \"1 0 0\" :\n",
    "#             n_inner_outer += 1\n",
    "#     elif labels[i] == \"1 0 0\":\n",
    "#         if labels[i] == predict_labels[i]:\n",
    "#             t_outer += 1 \n",
    "#         elif predict_labels[i] == \"0 0 1\" :\n",
    "#             n_outer_bounary += 1\n",
    "#         elif predict_labels[i] == \"0 1 0\" :\n",
    "#             n_outer_inner += 1\n",
    "# print(str(t_bounary) + \" \" + str(n_bounary_inner) + \" \" + str(n_bounary_outer))\n",
    "# print(str(n_inner_bounary) + \" \" + str(t_inner) + \" \" + str(n_inner_outer))\n",
    "# print(str(n_outer_bounary) + \" \" + str( n_outer_inner) + \" \" + str(n_bounary_outer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation import segmentation\n",
    "loss = []\n",
    "loss_per_sweep = []\n",
    "error = []\n",
    "def load_config2(src) :\n",
    "    filter, stride, train_per_sweep = [5, 2, 500]\n",
    "    filters = [16, 32, 64]\n",
    "    reader_train, num_samples_train = load_train(src)\n",
    "    reader_test, num_samples_test = load_test(src)\n",
    "    print(\"train samples =>\" + str(num_samples_train), \"test samples =>\" + str(num_samples_test))\n",
    "    \n",
    "    return filter, stride, filters,  train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test\n",
    "def process3(tissue, src_testing_data):\n",
    "    s = datetime.now(timezone('Asia/Bangkok')).strftime('log_date_%Y-%m-%d_time_%H-%M-%S')\n",
    "    \n",
    "    error_rates = []\n",
    "    sum_error_rate = 0\n",
    "    \n",
    "    src_root = \"../training_data/features/\"\n",
    "    src = src_root + tissue + \"/\"\n",
    "    \n",
    "    dst_root = \"../training_data/model/\"\n",
    "    dst = dst_root + tissue + \"/\"\n",
    "    \n",
    "    create_dir(dst_root)\n",
    "    create_dir(dst)\n",
    "    \n",
    "    path_log = dst + s  + \".txt\"\n",
    "    file_writer = open(path_log, 'w')\n",
    "    \n",
    "    for index in range(1,2):\n",
    "        src_k =  src + \"k-\" + str(index+1) + \"/\"\n",
    "        filter, stride, filters,  train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test = load_config2(src_k)\n",
    "        \n",
    "        \n",
    "        string_filter_details = \"folder => {:s}, train_per_sweep => {:d}\\nfilter => {:d}, stride => {:d}, dept => {:d}, {:d} {:d} {:d}\".format(src_k, train_per_sweep, filter, stride, len(filters), filters[0], filters[1], filters[2])\n",
    "        string_append_and_print(string_filter_details, path_log)\n",
    "        \n",
    "        error_rate, l, ls,e = do_train_test(reader_train, reader_test, input_dim, index+1, path_log, num_output_classes, filter, stride, filters, train_per_sweep, num_samples_train, num_samples_test, dst)\n",
    "        \n",
    "        segmentation().segment1(dst, \"model\"+str(error_rate)+\".model\", src_testing_data)\n",
    "    \n",
    "# process1(\"granulation\")\n",
    "src_testing_data = '../testing_data/granulation/'\n",
    "process3(\"augmen_motoC_to_s8_and_s8_to_iPad/granulation\", src_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation import segmentation\n",
    "loss = []\n",
    "loss_per_sweep = []\n",
    "error = []\n",
    "def load_config3(src) :\n",
    "    filter, stride, train_per_sweep = [5, 2, 1000]\n",
    "    filters = [16, 32, 64]\n",
    "    reader_train, num_samples_train = load_train(src)\n",
    "    reader_test, num_samples_test = load_test(src)\n",
    "    print(\"train samples =>\" + str(num_samples_train), \"test samples =>\" + str(num_samples_test))\n",
    "    \n",
    "    return filter, stride, filters,  train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test\n",
    "def process4(tissue, src_testing_data):\n",
    "    s = datetime.now(timezone('Asia/Bangkok')).strftime('log_date_%Y-%m-%d_time_%H-%M-%S')\n",
    "    \n",
    "    error_rates = []\n",
    "    sum_error_rate = 0\n",
    "    \n",
    "    src_root = \"../training_data/features/\"\n",
    "    src = src_root + tissue + \"/\"\n",
    "    \n",
    "    dst_root = \"../training_data/model/\"\n",
    "    dst = dst_root + tissue + \"/\"\n",
    "    \n",
    "    create_dir(dst_root)\n",
    "    create_dir(dst)\n",
    "    \n",
    "    path_log = dst + s  + \".txt\"\n",
    "    file_writer = open(path_log, 'w')\n",
    "    \n",
    "    for index in range(1,2):\n",
    "        src_k =  src + \"k-\" + str(index+1) + \"/\"\n",
    "        filter, stride, filters,  train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test = load_config3(src_k)\n",
    "        \n",
    "        \n",
    "        string_filter_details = \"folder => {:s}, train_per_sweep => {:d}\\nfilter => {:d}, stride => {:d}, dept => {:d}, {:d} {:d} {:d}\".format(src_k, train_per_sweep, filter, stride, len(filters), filters[0], filters[1], filters[2])\n",
    "        string_append_and_print(string_filter_details, path_log)\n",
    "        \n",
    "        error_rate, l, ls,e = do_train_test(reader_train, reader_test, input_dim, index+1, path_log, num_output_classes, filter, stride, filters, train_per_sweep, num_samples_train, num_samples_test, dst)\n",
    "        \n",
    "        segmentation().segment1(dst, \"model\"+str(error_rate)+\".model\", src_testing_data)\n",
    "    \n",
    "# process1(\"granulation\")\n",
    "src_testing_data = '../testing_data/granulation/'\n",
    "process4(\"augmen_motoC_to_s8_and_s8_to_iPad/granulation\", src_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
