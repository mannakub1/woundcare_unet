{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU[0] Tesla M60, CPU)\n"
     ]
    }
   ],
   "source": [
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "import cntk_unet\n",
    "from cntk.learners import learning_rate_schedule, UnitType\n",
    "from cntk.device import try_set_default_device, gpu, all_devices\n",
    "print(all_devices())\n",
    "try_set_default_device(gpu(0))\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix the random seed so that LR examples are repeatable\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import mathfrom datetime import datetime\n",
    "from pytz import timezone\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from cntk.initializer import normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import cntk\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from cntk.initializer import normal\n",
    "global f \n",
    "global index_model\n",
    "index_model = 1\n",
    "model_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "from cntk.layers import Convolution, MaxPooling, Dense\n",
    "from cntk.initializer import glorot_uniform\n",
    "from cntk.ops import relu, sigmoid, input_variable\n",
    "\n",
    "def UpSampling2D(x):\n",
    "    xr = C.reshape(x, (x.shape[0], x.shape[1], 1, x.shape[2], 1))\n",
    "    xx = C.splice(xr, xr, axis=-1) # axis=-1 refers to the last axis\n",
    "    xy = C.splice(xx, xx, axis=-3) # axis=-3 refers to the middle axis\n",
    "    r = C.reshape(xy, (x.shape[0], x.shape[1] * 2, x.shape[2] * 2))\n",
    "\n",
    "    return r\n",
    "\n",
    "def create_model(input, num_classes):\n",
    "    conv1 = Convolution((3,3), 32, init=glorot_uniform(), activation=relu, pad=True)(input)\n",
    "    conv1 = Convolution((3,3), 32, init=glorot_uniform(), activation=relu, pad=True)(conv1)\n",
    "    pool1 = MaxPooling((2,2), strides=(2,2))(conv1)\n",
    "\n",
    "    conv2 = Convolution((3,3), 64, init=glorot_uniform(), activation=relu, pad=True)(pool1)\n",
    "    conv2 = Convolution((3,3), 64, init=glorot_uniform(), activation=relu, pad=True)(conv2)\n",
    "    pool2 = MaxPooling((2,2), strides=(2,2))(conv2)\n",
    "\n",
    "    conv3 = Convolution((3,3), 128, init=glorot_uniform(), activation=relu, pad=True)(pool2)\n",
    "    conv3 = Convolution((3,3), 128, init=glorot_uniform(), activation=relu, pad=True)(conv3)\n",
    "    pool3 = MaxPooling((2,2), strides=(2,2))(conv3)\n",
    "\n",
    "    conv4 = Convolution((3,3), 256, init=glorot_uniform(), activation=relu, pad=True)(pool3)\n",
    "    conv4 = Convolution((3,3), 256, init=glorot_uniform(), activation=relu, pad=True)(conv4)\n",
    "    pool4 = MaxPooling((2,2), strides=(2,2))(conv4)\n",
    "\n",
    "    conv5 = Convolution((3,3), 512, init=glorot_uniform(), activation=relu, pad=True)(pool4)\n",
    "    conv5 = Convolution((3,3), 512, init=glorot_uniform(), activation=relu, pad=True)(conv5)\n",
    "   \n",
    "    up6 = C.splice(UpSampling2D(conv5), conv4, axis=0)\n",
    "    conv6 = Convolution((3,3), 256, init=glorot_uniform(), activation=relu, pad=True)(up6)\n",
    "    conv6 = Convolution((3,3), 256, init=glorot_uniform(), activation=relu, pad=True)(conv6)\n",
    "\n",
    "    up7 = C.splice(UpSampling2D(conv6), conv3, axis=0)\n",
    "    conv7 = Convolution((3,3), 128, init=glorot_uniform(), activation=relu, pad=True)(up7)\n",
    "    conv7 = Convolution((3,3), 128, init=glorot_uniform(), activation=relu, pad=True)(conv7)\n",
    "\n",
    "    up8 = C.splice(UpSampling2D(conv7), conv2, axis=0)\n",
    "    conv8 = Convolution((3,3), 64, init=glorot_uniform(), activation=relu, pad=True)(up8)\n",
    "    conv8 = Convolution((3,3), 64, init=glorot_uniform(), activation=relu, pad=True)(conv8)\n",
    "\n",
    "    up9 = C.splice(UpSampling2D(conv8), conv1, axis=0)\n",
    "    conv9 = Convolution((3,3), 64, init=glorot_uniform(), activation=relu, pad=True)(up9)\n",
    "    conv9 = Convolution((3,3), 64, init=glorot_uniform(), activation=relu, pad=True)(conv9)\n",
    "\n",
    "    conv10 = Convolution((1,1), num_classes, init=glorot_uniform(), activation=sigmoid, pad=True)(conv9)\n",
    "\n",
    "    return conv10\n",
    "\n",
    "def dice_coefficient(x, y):\n",
    "    # average of per-channel dice coefficient\n",
    "    # global dice coefificnet doesn't work as class with larger region dominates the metrics\n",
    "    # https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "    intersection = C.reduce_sum(x * y, axis=(1,2))\n",
    "    dice = C.reduce_mean(2.0 * intersection / (C.reduce_sum(x, axis=(1,2)) + C.reduce_sum(y, axis=(1,2)) + 1.0))\n",
    "    print(\"dice = \" + str(dice))\n",
    "\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_minibatch(all_features, all_labels, i, minibatch_size):\n",
    "    batch_features = all_features[i * minibatch_size:(i + 1) * minibatch_size]\n",
    "    batch_labels = all_labels[i * minibatch_size:(i + 1) * minibatch_size]\n",
    "\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "def measure_error(data_x, data_y, x, y, trainer, minibatch_size):\n",
    "    errors = []\n",
    "    for i in range(0, int(len(data_x) / minibatch_size)):\n",
    "        data_sx, data_sy = slice_minibatch(data_x, data_y, i, minibatch_size)\n",
    "\n",
    "        errors.append(trainer.test_minibatch({x: data_sx, y: data_sy}))\n",
    "\n",
    "    return np.mean(errors)\n",
    "\n",
    "def train(images, masks, dst_folder, file_writer, use_existing=False):\n",
    "    shape = images[0].shape\n",
    "    data_size = images.shape[0]\n",
    "    print(shape)\n",
    "    print(data_size)\n",
    "\n",
    "    # Split data\n",
    "    test_portion = int(data_size * 0.1)\n",
    "    indices = np.random.permutation(data_size)\n",
    "    test_indices = indices[:test_portion]\n",
    "    training_indices = indices[test_portion:]\n",
    "\n",
    "    test_data = (images[test_indices], masks[test_indices])\n",
    "    training_data = (images[training_indices], masks[training_indices])\n",
    "\n",
    "    # Create model\n",
    "    x = C.input_variable(shape)\n",
    "    y = C.input_variable(masks[0].shape)\n",
    "\n",
    "    z = create_model(x, masks.shape[1])\n",
    "    dice_coef = dice_coefficient(z, y)\n",
    "\n",
    "    # Load the saved model if specified\n",
    "    checkpoint_file = \"cntk-unet.dnn\"\n",
    "    if use_existing:\n",
    "        z.load_model(checkpoint_file)\n",
    "\n",
    "    # Prepare model and trainer\n",
    "    lr = learning_rate_schedule(0.00001, UnitType.sample)\n",
    "    momentum = C.learners.momentum_as_time_constant_schedule(0.9)\n",
    "    trainer = C.Trainer(z, (-dice_coef, -dice_coef), C.learners.adam(z.parameters, lr=lr, momentum=momentum))\n",
    "\n",
    "    # Get minibatches of training data and perform model training\n",
    "    minibatch_size = 8\n",
    "    num_epochs = 75\n",
    "\n",
    "    training_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for e in range(0, num_epochs):\n",
    "        for i in range(0, int(len(training_data[0]) / minibatch_size)):\n",
    "            batch_features, batch_labels = slice_minibatch(training_data[0], training_data[1], i, minibatch_size)\n",
    "\n",
    "            trainer.train_minibatch({x:batch_features, y: batch_labels})\n",
    "\n",
    "        # Measure training error\n",
    "        training_error = measure_error(training_data[0], training_data[1], x, y, trainer, minibatch_size)\n",
    "        training_errors.append(training_error)\n",
    "\n",
    "        # Measure test error\n",
    "        test_error = measure_error(test_data[0], test_data[1], x, y, trainer, minibatch_size)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "        string_log = \"epoch #{}: training_error={}, test_error={}\".format(e, training_errors[-1], test_errors[-1])\n",
    "        string_append_and_print(string_log, file_writer)\n",
    "        \n",
    "#         string_saved_model = \"\\t\\tmodel saved from data set {0:.2f}\".format(test_error*100 / num_minibatches_to_test)\n",
    "#         string_append_and_print(string_saved_model, path_log)\n",
    "        \n",
    "        trainer.save_checkpoint(checkpoint_file)\n",
    "#         z.save(dst_folder + \"/model{0:.2f}.model\".format(test_result*100 / num_minibatches_to_test))\n",
    "    return trainer, training_errors, test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_img_array(img_array, ncol=3):\n",
    "    nrow = len(img_array) // ncol\n",
    "\n",
    "    f, plots = plt.subplots(nrow, ncol, sharex='all', sharey='all', figsize=(ncol * 4, nrow * 4))\n",
    "\n",
    "    for i in range(len(img_array)):\n",
    "        plots[i // ncol, i % ncol]\n",
    "        plots[i // ncol, i % ncol].imshow(img_array[i])\n",
    "\n",
    "from functools import reduce\n",
    "def plot_side_by_side(img_arrays):\n",
    "    flatten_list = reduce(lambda x,y: x+y, zip(*img_arrays))\n",
    "\n",
    "    plot_img_array(np.array(flatten_list), ncol=len(img_arrays))\n",
    "\n",
    "import itertools\n",
    "def plot_errors(results_dict, title):\n",
    "    markers = itertools.cycle(('+', 'x', 'o'))\n",
    "\n",
    "    plt.title('{}'.format(title))\n",
    "\n",
    "    for label, result in sorted(results_dict.items()):\n",
    "        plt.plot(result, marker=next(markers), label=label)\n",
    "        plt.ylabel('dice_coef')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc=3, bbox_to_anchor=(1, 0))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def masks_to_colorimg(masks):\n",
    "    colors = np.asarray([(201, 58, 64), (242, 207, 1), (0, 152, 75), (101, 172, 228),(56, 34, 132), (160, 194, 56)])\n",
    "\n",
    "    colorimg = np.ones((masks.shape[1], masks.shape[2], 3), dtype=np.float32) * 255\n",
    "    channels, height, width = masks.shape\n",
    "\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            selected_colors = colors[masks[:,y,x] > 0.5]\n",
    "\n",
    "            if len(selected_colors) > 0:\n",
    "                colorimg[y,x,:] = np.mean(selected_colors, axis=0)\n",
    "\n",
    "    return colorimg.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reader_text(path, is_training, input_dim, num_label_classes):   \n",
    "    print(path)\n",
    "    labelStream = cntk.io.StreamDef(field='label', shape=(256, 256), is_sparse=False)\n",
    "    featureStream = cntk.io.StreamDef(field='features', shape=(3, 256, 256), is_sparse=False)\n",
    "#     coordinateStream = cntk.io.StreamDef(field='coordinate', shape=2, is_sparse=False)\n",
    "    \n",
    "#     deserailizer = cntk.io.CBFDeserializer(path, cntk.io.StreamDefs(label = labelStream ,feature = featureStream))\n",
    "    deserailizer = cntk.io.CTFDeserializer(path, cntk.io.StreamDefs(label = labelStream ,feature = featureStream))      \n",
    "    return cntk.io.MinibatchSource(deserailizer,\n",
    "       randomize = False, multithreaded_deserializer=False,max_sweeps = cntk.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "def create_reader_binary(path, is_training, input_dim, num_label_classes):   \n",
    "#     print(path)\n",
    "    labelStream = cntk.io.StreamDef(field='label', shape=num_label_classes, is_sparse=False)\n",
    "    featureStream = cntk.io.StreamDef(field='features', shape=input_dim, is_sparse=False)\n",
    "#     coordinateStream = cntk.io.StreamDef(field='coordinate', shape=2, is_sparse=False)\n",
    "    \n",
    "    deserailizer = cntk.io.CBFDeserializer(path, cntk.io.StreamDefs(label = labelStream ,feature = featureStream))\n",
    "#     deserailizer = cntk.io.CTFDeserializer(path, cntk.io.StreamDefs(label = labelStream ,feature = featureStream))      \n",
    "    return cntk.io.MinibatchSource(deserailizer,\n",
    "       randomize = False, multithreaded_deserializer=False,max_sweeps = cntk.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_criterion_function(model, labels):\n",
    "    loss = cntk.cross_entropy_with_softmax(model, labels)\n",
    "    errs = cntk.classification_error(model, labels)\n",
    "    return loss, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "#     if mb%frequency == 0:\n",
    "    training_loss = trainer.previous_minibatch_loss_average\n",
    "    eval_error = trainer.previous_minibatch_evaluation_average\n",
    "#         if verbose: \n",
    "    string_timing = \"\\t\\tMinibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100)\n",
    "    string_append_and_print(string_timing, path_log)\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_adaptive_learner(model, base_lr_per_sample, minibatch_size, num_samples_per_sweep):\n",
    "    lr_schedule = get_adaptive_learning_schedule(base_lr_per_sample, minibatch_size, num_samples_per_sweep)\n",
    "    #momentum_time_constant = -minibatch_size / np.log(0.98)\n",
    "    #momentum_time_constant = [3000]\n",
    "    l2_reg_weight = 0.0001  # 0.0001\n",
    "    #mm_schedule = cntk.momentum_as_time_constant_schedule(momentum_time_constant)\n",
    "    mm_schedule = cntk.momentum_schedule(0.90)\n",
    "\n",
    "    learner = cntk.momentum_sgd(model.parameters, lr_schedule, mm_schedule,\n",
    "                           l2_regularization_weight=l2_reg_weight, unit_gain=True)\n",
    "\n",
    "    return learner\n",
    "    \n",
    "def get_adaptive_learning_schedule(base_lr_per_sample, minibatch_size, num_samples_per_sweep):\n",
    "    return cntk.learners.learning_parameter_schedule(base_lr_per_sample, minibatch_size, num_samples_per_sweep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_append_and_print(line, file) :\n",
    "    file_writer = open(file, 'a')\n",
    "    file_writer.write(line + \"\\n\")\n",
    "    file_writer.close()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(plotdata):\n",
    "    plt.plot(plotdata[\"mb\"], plotdata[\"loss\"], '--r')\n",
    "    plt.xlabel('Minibatch number')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_model = (3, 256, 256)\n",
    "input_dim = 3 * 256 * 256\n",
    "num_output_classes = 256 * 256\n",
    "input = cntk.input_variable(input_dim_model)  # สังเกตว่าเราใช้ input_dim_model เป็นพารามิเตอร์แทนการใช้ input_dim\n",
    "label = cntk.input_variable((256,256))\n",
    "\n",
    "# samples_test_size = len(open(\"../training_data/features/original/samples_test.txt\", \"r\").readlines())\n",
    "samples_train_size = len(open(\"../training_data/features/original/samples_train.txt\", \"r\").readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_test(train_reader, test_reader, model_func ,path_log, num_sweeps_to_train_with, num_samples_train, num_samples_test, dst) :\n",
    "    model = model_func(input/255)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function(model, label)\n",
    "    \n",
    "    minibatch_size = 64\n",
    "\n",
    "    num_samples_per_sweep = num_samples_train\n",
    "\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "    \n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    k = int(num_samples_per_sweep / minibatch_size)\n",
    "    num_k = num_sweeps_to_train_with // 5\n",
    "    learning_rate = ([0.2]*(k*num_k))+([0.1]*(k*num_k))+([0.05]*(k*num_k))+([0.025]*(k*num_k))+([0.0125]*(k*num_k))\n",
    "#     learning_rate = [0.2] * k\n",
    "    lr_schedule = cntk.learning_rate_schedule(learning_rate, cntk.UnitType.minibatch)\n",
    "    learner =  set_adaptive_learner(z, learning_rate, minibatch_size, num_samples_per_sweep)\n",
    "#     learner = cntk.sgd(z.parameters, lr_schedule)\n",
    "#     dice_coef = dice_coefficient(z, label)\n",
    "    trainer = cntk.Trainer(z, (loss, label_error), [learner])\n",
    "    \n",
    "    input_map={\n",
    "        label  : train_reader.streams.label,\n",
    "        input  : train_reader.streams.feature\n",
    "    }  \n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    training_progress_output_freq = 0\n",
    "     \n",
    "    # Start a timer\n",
    "    start = time.time()\n",
    "    l_list = []\n",
    "    error = []\n",
    "    loss_per_sweep = []\n",
    "    for i in range(0, int(num_minibatches_to_train)):\n",
    "\n",
    "        # Read a mini batch from the training data file\n",
    "        data = train_reader.next_minibatch(minibatch_size, input_map=input_map) \n",
    "        \n",
    "        training_data = []\n",
    "        for k in data.keys() :\n",
    "            for v in data[k].as_sequences()[0] :\n",
    "                 training_data.append(v)\n",
    "\n",
    "        data_x, data_y = training_data\n",
    "        data_x = np.reshape(data_x,(256,256))\n",
    "        data_y = np.reshape(data_y,(3,256,256))\n",
    "\n",
    "#         data_x, data_y \n",
    "\n",
    "        trainer.train_minibatch({input: data_y, label: data_x})\n",
    "\n",
    "#         a, l, e = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "#         a, l, e = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "        if i % training_progress_output_freq == 0 :\n",
    "# #             print(i)\n",
    "            a, l, e = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "            l_list.append(l)\n",
    "            error.append(e)\n",
    "        if i % k == 0 :\n",
    "            \n",
    "            a, l, e = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "            string_training_time = \"\\t\\ttraining took per sweep round = {:.0f} => {:.1f} sec\".format((i+1)/k,time.time() - start)\n",
    "            string_append_and_print(string_training_time, path_log)\n",
    "            plotdata[\"mb\"].append(i)\n",
    "            plotdata[\"loss\"].append(l)\n",
    "            loss_per_sweep.append(l)\n",
    "#             print(\"loss per sweep to train = \" + str(l))\n",
    "       \n",
    "     \n",
    "    # Print training time\n",
    "    string_training_time = \"\\t\\tTraining took {:.1f} sec\".format(time.time() - start)\n",
    "    string_append_and_print(string_training_time, path_log)\n",
    "    \n",
    "    # Test the model\n",
    "    test_input_map = {\n",
    "        label  : test_reader.streams.label,\n",
    "        input  : test_reader.streams.feature\n",
    "    }\n",
    "\n",
    "    # Test data for trained model\n",
    "    test_minibatch_size = 64\n",
    "    \n",
    "#     num_samples = len( open(name,'r').readlines() )\n",
    "    num_samples = num_samples_test\n",
    "    num_minibatches_to_test = num_samples_test // test_minibatch_size\n",
    "    \n",
    "    test_result = 0.0   \n",
    "\n",
    "    for i in range(num_minibatches_to_test):\n",
    "    \n",
    "        # We are loading test data in batches specified by test_minibatch_size\n",
    "        # Each data point in the minibatch is a MNIST digit image of 784 dimensions \n",
    "        # with one pixel per dimension that we will encode / decode with the \n",
    "        # trained model.\n",
    "        data = test_reader.next_minibatch(test_minibatch_size, input_map=test_input_map)\n",
    "        \n",
    "       \n",
    "        # reshpae data this here.\n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "        test_result = test_result + eval_error\n",
    "    \n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    string_average_test_error = \"\\t\\tAverage test error: {0:.2f}%\".format(test_result*100 / num_minibatches_to_test)\n",
    "    string_append_and_print(string_average_test_error, path_log)\n",
    "  \n",
    "    #save model whem error late less number target\n",
    "    path_model = dst + \"/model{0:.2f}.model\".format(test_result*100 / num_minibatches_to_test)\n",
    "#     if((test_result*100 / num_minibatches_to_test) < 9) :\n",
    "    string_saved_model = \"\\t\\tmodel saved from data set {0:.2f}\".format(test_result*100 / num_minibatches_to_test)\n",
    "    string_append_and_print(string_saved_model, path_log)\n",
    "    model_list.append(z)\n",
    "#     if (test_result*100 / num_minibatches_to_test) <= 8.00 :\n",
    "    z.save(path_model)\n",
    "    return \"{0:.2f}\".format(test_result*100 / num_minibatches_to_test), l_list, loss_per_sweep,error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(data):\n",
    "    training_data = []\n",
    "    for k in data.keys() :\n",
    "        for v in data[k].as_sequences()[0] :\n",
    "             training_data.append(v)\n",
    "\n",
    "def reshape(data, shape) :\n",
    "    return np.reshape(data, shape)\n",
    "\n",
    "# def prepare_data(reader_train, reader_test) :\n",
    "#     input_map={\n",
    "#         label  : train_reader.streams.label,\n",
    "#         input  : train_reader.streams.feature\n",
    "#     } \n",
    "#     reader_train.next_minibatch(minibatch_size, input_map=input_map) \n",
    "    \n",
    "        \n",
    "#         data_x, data_y = training_data\n",
    "#         data_x = np.reshape(data_x,(256,256))\n",
    "#         data_y = np.reshape(data_y,(3,256,256))\n",
    "        \n",
    "#     test_input_map = {\n",
    "#         label  : test_reader.streams.label,\n",
    "#         input  : test_reader.streams.feature\n",
    "#     }\n",
    "    \n",
    "\n",
    "def do_train_test(reader_train, reader_test, input_dim, path_log ,num_output_classes, num_sweeps_to_train_with, num_samples_train, num_samples_test, dst):\n",
    "    global z\n",
    "    z = create_model(input, 256*256)\n",
    "    \n",
    "#     return train(reader_train, reader_test, dst, path_log)\n",
    "    return train_test(reader_train, reader_test, z, path_log, num_sweeps_to_train_with, num_samples_train, num_samples_test,dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_test(src) :\n",
    "    src_test_file = src + \"samples_test.dat\"\n",
    "    num_samples = samples_test_size\n",
    "    \n",
    "    return create_reader_binary(src_test_file, False, input_dim, num_output_classes), num_samples\n",
    "\n",
    "def load_train(src) :\n",
    "    src_train_file = src + \"samples_train.dat\"\n",
    "    num_samples = samples_train_size\n",
    "        \n",
    "    return create_reader_binary(src_train_file, True, input_dim, num_output_classes), num_samples\n",
    "    \n",
    "def load_config(src) :\n",
    "    train_per_sweep = 10\n",
    "    reader_train, num_samples_train = load_train(src)\n",
    "    reader_test, num_samples_test = load_test(src)\n",
    "    \n",
    "    return train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder_exist(src_folder) :\n",
    "    return os.path.exists(src_folder)\n",
    "\n",
    "def create_folder(src_folder) :\n",
    "    folders = src_folder.split(\"/\")\n",
    "    current_folder = \"\"\n",
    "    \n",
    "    for folder in folders :\n",
    "        current_folder += folder + \"/\"\n",
    "        if not (folder == '' or folder == '.' or folder == '..') and not (check_folder_exist(current_folder)):\n",
    "            os.mkdir(current_folder)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def process(src_folder, dst_folder) :\n",
    "    \n",
    "    create_folder(dst_folder)\n",
    "    time = datetime.now(timezone('Asia/Bangkok')).strftime('log_date_%Y-%m-%d_time_%H-%M-%S')\n",
    "    \n",
    "    path_log = dst_folder + time + \".txt\"\n",
    "    file_writer = open(path_log, 'w')\n",
    "    \n",
    "    train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test = load_config(src_folder)\n",
    "    string_filter_details = \"folder => {:s}, train_per_sweep => {:d}\\n\".format(src_folder, train_per_sweep)\n",
    "    \n",
    "    \n",
    "#     return do_train_test(reader_train, reader_test, input_dim, path_log, num_output_classes, train_per_sweep, num_samples_train, num_samples_test, dst_folder)\n",
    "    return reader_train, reader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples_test_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d05069f58039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdst_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../training_data/model/original/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# process(src_folder, dst_folder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-545ed82e2cb5>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(src_folder, dst_folder)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfile_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_per_sweep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mstring_filter_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"folder => {:s}, train_per_sweep => {:d}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_per_sweep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c5816213237f>\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(src)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_per_sweep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mreader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mreader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_per_sweep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c5816213237f>\u001b[0m in \u001b[0;36mload_test\u001b[0;34m(src)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msrc_test_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"samples_test.dat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples_test_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_reader_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_test_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'samples_test_size' is not defined"
     ]
    }
   ],
   "source": [
    "src_folder = \"../training_data/features/original/\"\n",
    "dst_folder = \"../training_data/model/original/\"\n",
    "# process(src_folder, dst_folder)\n",
    "t, t1, t2 = process(src_folder, dst_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1166c7a66499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "print(x.shape[0])\n",
    "print(y.shape[0])\n",
    "\n",
    "print(reader_train.streams.label)\n",
    "data_x = np.reshape(x,(256,256))\n",
    "data_y = np.reshape(y,(3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5bc6697e1631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_y' is not defined"
     ]
    }
   ],
   "source": [
    "print(data_y[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-659ade3690f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'temp_y' is not defined"
     ]
    }
   ],
   "source": [
    "print(temp_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label([65536])\n",
      "feature([196608])\n"
     ]
    }
   ],
   "source": [
    "src_folder = \"../training_data/features/original/\"\n",
    "dst_folder = \"../training_data/model/original/\"\n",
    "reader_train, reader_test = load_train(src_folder)\n",
    "minibatch_size = 1\n",
    "num_samples_per_sweep = samples_train_size\n",
    "num_minibatches_to_train = (num_samples_per_sweep ) / minibatch_size\n",
    "\n",
    "\n",
    "# input_map={\n",
    "#         label  : cntk.input_variable((256, 256)),\n",
    "#         input  : cntk.input_variable((126,3,256,256))\n",
    "#     }\n",
    "\n",
    "input_map={\n",
    "        label  : reader_train.streams.label,\n",
    "        input  : reader_train.streams.feature\n",
    "} \n",
    "\n",
    "print(reader_train.streams.label)\n",
    "print(reader_train.streams.feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(x, y, index) :\n",
    "    data_x = np.reshape(x,(256,256))\n",
    "    data_y = np.reshape(y,(3,256,256))\n",
    "    print(data_y)\n",
    "    print(\">>>>>>>>>>>> \" + str(index))\n",
    "\n",
    "data_x, data_y = [], []\n",
    "for i in range(0, int(num_minibatches_to_train)):\n",
    "    \n",
    "    # Read a mini batch from the training data file\n",
    "    data = reader_train.next_minibatch(minibatch_size, input_map=input_map) \n",
    "#     if i < 30:\n",
    "    training_data = []\n",
    "    for k in data.keys() :\n",
    "        for v in data[k].as_sequences()[0] :\n",
    "             training_data.append(v)\n",
    "\n",
    "    data_x.append(training_data[0])\n",
    "    data_y.append(training_data[1])\n",
    "#     reshape(data_x, data_y, i+1)\n",
    "#     else : \n",
    "#         break\n",
    "#             data_x = np.reshape(data_x,(256,256))\n",
    "#             data_y = np.reshape(data_y,(3,256,256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.reshape(data_x, (180,1,256,256))\n",
    "features = np.reshape(data_y,(180,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 1, 256, 256)\n",
      "(180, 3, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "print(features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "create_folder(dst_folder)\n",
    "time = datetime.now(timezone('Asia/Bangkok')).strftime('log_date_%Y-%m-%d_time_%H-%M-%S')\n",
    "\n",
    "path_log = dst_folder + time + \".txt\"\n",
    "file_writer = open(path_log, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 256, 256)\n",
      "180\n",
      "dice = Composite(Tensor[3,256,256], Tensor[1,256,256]) -> np.float32\n",
      "epoch #0: training_error=-0.29813248962163924, test_error=-0.30808837711811066\n",
      "epoch #1: training_error=-0.408703538775444, test_error=-0.42861929535865784\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer, training_errors, test_errors = train(features, labels, dst_folder, path_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_errors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-37a83ca0b2ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_errors\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Simulation Learning Curve\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_errors' is not defined"
     ]
    }
   ],
   "source": [
    "plot_errors({\"training\": training_errors, \"test\": test_errors}, title=\"Simulation Learning Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z.save(dst_folder + \"/model{0:.2f}.model\".format(test_result*100 / num_minibatches_to_test))\n",
    "z.save(dst_folder + time)\n",
    "# test_errors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors({\"training\": training_errors, \"test\": test_errors}, title=\"Simulation Learning Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_train.stream_infos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reader_train.next_minibatch(minibatch_size, input_map=input_map) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for k in data.keys() :\n",
    "    for v in data[k].as_sequences()[0] :\n",
    "         print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation import segmentation\n",
    "loss = []\n",
    "loss_per_sweep = []\n",
    "error = []\n",
    "\n",
    "def process1(tissue, src_testing_data):\n",
    "    s = datetime.now(timezone('Asia/Bangkok')).strftime('log_date_%Y-%m-%d_time_%H-%M-%S')\n",
    "    \n",
    "    error_rates = []\n",
    "    sum_error_rate = 0\n",
    "    \n",
    "    src_root = \"../training_data/features/\"\n",
    "    src = src_root + tissue + \"/\"\n",
    "    \n",
    "    dst_root = \"../training_data/model/\"\n",
    "    dst = dst_root + tissue + \"/\"\n",
    "    \n",
    "    create_dir(dst_root)\n",
    "    create_dir(dst)\n",
    "    \n",
    "    path_log = dst + s  + \".txt\"\n",
    "    file_writer = open(path_log, 'w')\n",
    "    \n",
    "    for index in range(1,2):\n",
    "        src_k =  src + \"k-\" + str(index+1) + \"/\"\n",
    "        filter, stride, filters,  train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test = load_config(src_k)\n",
    "        \n",
    "        \n",
    "        string_filter_details = \"folder => {:s}, train_per_sweep => {:d}\\nfilter => {:d}, stride => {:d}, dept => {:d}, {:d} {:d} {:d}\".format(src_k, train_per_sweep, filter, stride, len(filters), filters[0], filters[1], filters[2])\n",
    "        string_append_and_print(string_filter_details, path_log)\n",
    "        \n",
    "        error_rate, l, ls,e = do_train_test(reader_train, reader_test, input_dim, index+1, path_log, num_output_classes, filter, stride, filters, train_per_sweep, num_samples_train, num_samples_test, dst)\n",
    "        \n",
    "        segmentation().segment1(dst, \"model\"+str(error_rate)+\".model\", src_testing_data)\n",
    "    \n",
    "# process1(\"granulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plotdata = defaultdict(list)\n",
    "src_testing_data = '../testing_data/granulation/'\n",
    "process1(\"/granulation\", src_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotdata = defaultdict(list)\n",
    "src_testing_data = '../testing_data/necrotic/'\n",
    "process1(\"necrotic\", src_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotdata = defaultdict(list)\n",
    "src_testing_data = '../testing_data/slough/'\n",
    "process1(\"/slough\", src_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotdata = defaultdict(list)\n",
    "# src_testing_data = '../testing_data/original/'\n",
    "# process1(\"/original\", src_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plotdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process1(\"granulation\")\n",
    "# plot_1000 = plotdata\n",
    "plot(plot_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process1(\"augmen_motoC_to_s8_and_s8_to_iPad/necrotic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process1(\"augmen_motoC_to_s8_and_s8_to_iPad/slough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process1(\"augmen_motoC_to_s8_and_s8_to_iPad/original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "loss_per_sweep = []\n",
    "error = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# def process1(filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test) :\n",
    "#     error_rates = []\n",
    "#     sum_error_rate = 0\n",
    "#     file_writer = open(path_log, 'a')\n",
    "    \n",
    "#     string_filter_details = \"filter => {:d}, stride => {:d}, dept => {:d}, {:d} {:d} {:d}\".format(filter, stride, len(filters), filters[0], filters[1], filters[2])\n",
    "#     string_append_and_print(string_filter_details, path_log)\n",
    "\n",
    "#     file_writer.close()\n",
    "#     file_writer = open(path_log, 'a')\n",
    "    \n",
    "#     #k-flod validation = 5\n",
    "    \n",
    "#     src_root = \"../training_data/features/\"\n",
    "#     src_granulation = src_root + \"granulation/\"\n",
    "    \n",
    "#     dst_root = \"../training_data/model/\"\n",
    "#     dst_granulation = dst_root + \"granulation/\"\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for index in range(5):\n",
    "#         #address file train and test\n",
    "#         src_granulation_k = \"k-\" + str(index+1)\n",
    "#         string_index_data_set = '\\tData Set : ' + str(i)\n",
    "#         string_append_and_print(string_index_data_set, path_log)\n",
    "        \n",
    "#         #read file train and test\n",
    "# #         train_file = os.path.join(data_dir, \"train.dat\")\n",
    "# #         test_file = os.path.join(data_dir, \"test.dat\")\n",
    "        \n",
    "#         #start train and test\n",
    "#         error_rate, l, ls,e = do_train_test(reader_train, reader_test, input_dim, i, file_writer, num_output_classes, filter, stride, filters, train_per_sweep, num_samples_train, num_samples_test)\n",
    "#         sum_error_rate += error_rate\n",
    "#         loss.append(l)\n",
    "#         loss_per_sweep.append(ls)\n",
    "#         error.append(e)\n",
    "#         #append error rate to list for calculate average error late\n",
    "#         error_rates.append(error_rate)\n",
    "\n",
    "#         if index_model > 1 :\n",
    "#             break\n",
    "\n",
    "#     string_file_end_line = \"\\tAverage model error: {0:.2f}%\\n\\tmin model error: {0:.2f}%\\n\\t>>>>>>>>>>>>>>>>>>\\n>>>>>>>>>>>>>>>>>>\".format(sum_error_rate/train_per_sweep,min(error_rates))     \n",
    "#     string_append_and_print(string_file_end_line, path_log)\n",
    "    \n",
    "#     file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_180_test_less_filter_per_10_sweep()\n",
    "# filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_180_test_less_filter_per_50_sweep()\n",
    "# filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_180_train_less_filter_per_10_sweep()\n",
    "# filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_180_train_more_filter_per_10_sweep()\n",
    "# filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_180_train_less_filter_per_50_sweep()\n",
    "# filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_14400_test_less_filter_per_10_sweep()\n",
    "filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test = load_config_14400_train_less_filter_per_10_sweep()\n",
    "open(path_log,'w')\n",
    "print(num_samples_train)\n",
    "print(num_samples_test)\n",
    "# process1(5, 2, [256, 512, 1024],  50, path_log)\n",
    "process1(filter, stride, filters, train_per_sweep, path_log, reader_train, reader_test, num_samples_train, num_samples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "# from cntk.ops.functions import load_model\n",
    "# current_model = load_model(\"output/model/model4.model\")\n",
    "# print(z)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(loss_per_sweep[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = list(range(0,len(loss_per_sweep[0])*60000,60000))\n",
    "plt.plot(mb, loss_per_sweep[0])\n",
    "plt.show()\n",
    "# plt.figure(1)\n",
    "# for l in range(len(loss_per_sweep)-1) :\n",
    "#     sub_plot = \"33\" + str((int(l) + 1))\n",
    "#     plt.subplot(int(sub_plot))\n",
    "#     plt.plot(mb, loss_per_sweep[l])\n",
    "#     plt.title(\"data set \" + str((int(l) + 1)))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = list(range(0,len(loss[0])*10000,10000))\n",
    "plt.figure(1)\n",
    "for l in range(len(loss)-1) :\n",
    "    sub_plot = \"33\" + str((int(l) + 1))\n",
    "    plt.subplot(int(sub_plot))\n",
    "    plt.plot(mb, loss[l])\n",
    "    plt.title(\"data set \" + str((int(l) + 1)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import TestingData\n",
    "testing_data, labels, size_file = TestingData().load(\"../testing_data/input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_label = []\n",
    "\n",
    "for i in range(0, size_file):\n",
    "    prediction_label.append(z.eval(testing_data[i])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_label(index) :\n",
    "    if index == 0 :\n",
    "        return \"1 0 0\"\n",
    "    elif index == 1 :\n",
    "        return \"0 1 0\"\n",
    "    else :\n",
    "        return \"0 0 1\"\n",
    "\n",
    "prediction_labels = []\n",
    "# path_file_prediction = \"output_label/prediction_label.txt\"\n",
    "# file = open(path_file_prediction, 'w')\n",
    "# file = open(path_file_prediction, 'a')\n",
    "\n",
    "for current_arr in prediction_label :\n",
    "# #     file.write(predict_label(np.argmax(current_arr))+\"\\n\")\n",
    "    prediction_labels.append(predict_label(np.argmax(current_arr)))\n",
    "# file.close()\n",
    "\n",
    "size_labels = len(labels)\n",
    "t_bounary, t_inner, t_outer = 0, 0, 0\n",
    "n_bounary_inner, n_bounary_outer = 0, 0\n",
    "n_inner_bounary, n_inner_outer = 0, 0\n",
    "n_outer_bounary, n_outer_inner = 0, 0,\n",
    "for i in range(0, size_labels) :\n",
    "    if labels[i] == \"0 0 1\":\n",
    "        if labels[i] == prediction_labels[i]:\n",
    "            t_bounary += 1 \n",
    "        elif prediction_labels[i] == \"0 1 0\" :\n",
    "            n_bounary_inner += 1\n",
    "        elif prediction_labels[i] == \"1 0 0\" :\n",
    "            n_bounary_outer += 1\n",
    "    elif labels[i] == \"0 1 0\":\n",
    "        if labels[i] == prediction_labels[i]:\n",
    "            t_inner += 1 \n",
    "        elif prediction_labels[i] == \"0 0 1\" :\n",
    "            n_inner_bounary += 1\n",
    "        elif prediction_labels[i] == \"1 0 0\" :\n",
    "            n_inner_outer += 1\n",
    "    elif labels[i] == \"1 0 0\":\n",
    "        if labels[i] == prediction_labels[i]:\n",
    "            t_outer += 1 \n",
    "        elif prediction_labels[i] == \"0 0 1\" :\n",
    "            n_outer_bounary += 1\n",
    "        elif prediction_labels[i] == \"0 1 0\" :\n",
    "            n_outer_inner += 1\n",
    "print(str(t_bounary) + \" \" + str(n_bounary_inner) + \" \" + str(n_bounary_outer))\n",
    "print(str(n_inner_bounary) + \" \" + str(t_inner) + \" \" + str(n_inner_outer))\n",
    "print(str(n_outer_bounary) + \" \" + str( n_outer_inner) + \" \" + str(n_bounary_outer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform testing data for prediction\n",
    "\n",
    "# def number_of_line(path):\n",
    "#     file = open(path, 'r')\n",
    "#     f = []\n",
    "#     while True :\n",
    "#         line = file.readline();\n",
    "#         if not line :\n",
    "#             break\n",
    "#         f.append(line)\n",
    "#     return len(f)\n",
    "\n",
    "# def prediction_file(path_directory):\n",
    "#     arr = []\n",
    "    \n",
    "#     path_file = \"testing_features_data.txt\"\n",
    "#     size_file = number_of_line(path_directory + path_file)\n",
    "\n",
    "#     testing_file = os.path.join(path_directory, path_file)\n",
    "#     reader_test = create_reader(testing_file, False, input_dim, num_output_classes)\n",
    "#     test_input_map = {\n",
    "#             input  : reader_test.streams.features,\n",
    "#             label : reader_test.streams.labels\n",
    "#         }\n",
    "#     data = reader_test.next_minibatch(size_file, input_map=test_input_map)\n",
    "#     data_asarray =  data[input].asarray()\n",
    "\n",
    "#     patch = np.reshape(data_asarray[0], (3,31,31))\n",
    "#     print(z.eval(patch)[0])\n",
    "# #     z.eval(data_asarray)\n",
    "# #     for i in range(0, size_file):\n",
    "# #         patch = np.reshape(data_asarray[i], (3,31,31))\n",
    "# #         print(current_model.eval(patch)[0])\n",
    "# #         arr.append(current_model.eval(patch)[0])\n",
    "# #     print(len(arr))\n",
    "#     return arr\n",
    "\n",
    "# arr = prediction_file(\"../testing_data/input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #prediction and write answer in file \n",
    "\n",
    "# predict_labels = []\n",
    "\n",
    "# #prediction method => index : position label max\n",
    "# def predict_label(index) :\n",
    "#     if index == 0 :\n",
    "#         return \"1 0 0\"\n",
    "#     elif index == 1 :\n",
    "#         return \"0 1 0\"\n",
    "#     else :\n",
    "#         return \"0 0 1\"\n",
    "\n",
    "# path_file_prediction = \"output_label/prediction_label.txt\"\n",
    "# file = open(path_file_prediction, 'w')\n",
    "# file = open(path_file_prediction, 'a')\n",
    "\n",
    "# for current_arr in arr :\n",
    "#     file.write(predict_label(np.argmax(current_arr))+\"\\n\")\n",
    "#     predict_labels.append(predict_label(np.argmax(current_arr)))\n",
    "# file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test set data\n",
    "# labels = []\n",
    "# def get_label_from_testing_data(path):    \n",
    "#     file_test = open(path)\n",
    "#     while True :\n",
    "#         line = file_test.readline()\n",
    "#         if not line :\n",
    "#             break\n",
    "#         lines = line.split(\" \")\n",
    "#         current_label = lines[1] + \" \" + lines[2] + \" \" + lines[3]\n",
    "#         labels.append(current_label)\n",
    "\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t1.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t2.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t4.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t5.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t6.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t7.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t8.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t9.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t10.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_labels = len(labels)\n",
    "# t_bounary, t_inner, t_outer = 0, 0, 0\n",
    "# n_bounary_inner, n_bounary_outer = 0, 0\n",
    "# n_inner_bounary, n_inner_outer = 0, 0\n",
    "# n_outer_bounary, n_outer_inner = 0, 0,\n",
    "# for i in range(0, size_labels) :\n",
    "#     if labels[i] == \"0 0 1\":\n",
    "#         if labels[i] == predict_labels[i]:\n",
    "#             t_bounary += 1 \n",
    "#         elif predict_labels[i] == \"0 1 0\" :\n",
    "#             n_bounary_inner += 1\n",
    "#         elif predict_labels[i] == \"1 0 0\" :\n",
    "#             n_bounary_outer += 1\n",
    "#     elif labels[i] == \"0 1 0\":\n",
    "#         if labels[i] == predict_labels[i]:\n",
    "#             t_inner += 1 \n",
    "#         elif predict_labels[i] == \"0 0 1\" :\n",
    "#             n_inner_bounary += 1\n",
    "#         elif predict_labels[i] == \"1 0 0\" :\n",
    "#             n_inner_outer += 1\n",
    "#     elif labels[i] == \"1 0 0\":\n",
    "#         if labels[i] == predict_labels[i]:\n",
    "#             t_outer += 1 \n",
    "#         elif predict_labels[i] == \"0 0 1\" :\n",
    "#             n_outer_bounary += 1\n",
    "#         elif predict_labels[i] == \"0 1 0\" :\n",
    "#             n_outer_inner += 1\n",
    "# print(str(t_bounary) + \" \" + str(n_bounary_inner) + \" \" + str(n_bounary_outer))\n",
    "# print(str(n_inner_bounary) + \" \" + str(t_inner) + \" \" + str(n_inner_outer))\n",
    "# print(str(n_outer_bounary) + \" \" + str( n_outer_inner) + \" \" + str(n_bounary_outer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation import segmentation\n",
    "loss = []\n",
    "loss_per_sweep = []\n",
    "error = []\n",
    "def load_config2(src) :\n",
    "    filter, stride, train_per_sweep = [5, 2, 500]\n",
    "    filters = [16, 32, 64]\n",
    "    reader_train, num_samples_train = load_train(src)\n",
    "    reader_test, num_samples_test = load_test(src)\n",
    "    print(\"train samples =>\" + str(num_samples_train), \"test samples =>\" + str(num_samples_test))\n",
    "    \n",
    "    return filter, stride, filters,  train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test\n",
    "def process3(tissue, src_testing_data):\n",
    "    s = datetime.now(timezone('Asia/Bangkok')).strftime('log_date_%Y-%m-%d_time_%H-%M-%S')\n",
    "    \n",
    "    error_rates = []\n",
    "    sum_error_rate = 0\n",
    "    \n",
    "    src_root = \"../training_data/features/\"\n",
    "    src = src_root + tissue + \"/\"\n",
    "    \n",
    "    dst_root = \"../training_data/model/\"\n",
    "    dst = dst_root + tissue + \"/\"\n",
    "    \n",
    "    create_dir(dst_root)\n",
    "    create_dir(dst)\n",
    "    \n",
    "    path_log = dst + s  + \".txt\"\n",
    "    file_writer = open(path_log, 'w')\n",
    "    \n",
    "    for index in range(1,2):\n",
    "        src_k =  src + \"k-\" + str(index+1) + \"/\"\n",
    "        filter, stride, filters,  train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test = load_config2(src_k)\n",
    "        \n",
    "        \n",
    "        string_filter_details = \"folder => {:s}, train_per_sweep => {:d}\\nfilter => {:d}, stride => {:d}, dept => {:d}, {:d} {:d} {:d}\".format(src_k, train_per_sweep, filter, stride, len(filters), filters[0], filters[1], filters[2])\n",
    "        string_append_and_print(string_filter_details, path_log)\n",
    "        \n",
    "        error_rate, l, ls,e = do_train_test(reader_train, reader_test, input_dim, index+1, path_log, num_output_classes, filter, stride, filters, train_per_sweep, num_samples_train, num_samples_test, dst)\n",
    "        \n",
    "        segmentation().segment1(dst, \"model\"+str(error_rate)+\".model\", src_testing_data)\n",
    "    \n",
    "# process1(\"granulation\")\n",
    "src_testing_data = '../testing_data/granulation/'\n",
    "process3(\"augmen_motoC_to_s8_and_s8_to_iPad/granulation\", src_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation import segmentation\n",
    "loss = []\n",
    "loss_per_sweep = []\n",
    "error = []\n",
    "def load_config3(src) :\n",
    "    filter, stride, train_per_sweep = [5, 2, 1000]\n",
    "    filters = [16, 32, 64]\n",
    "    reader_train, num_samples_train = load_train(src)\n",
    "    reader_test, num_samples_test = load_test(src)\n",
    "    print(\"train samples =>\" + str(num_samples_train), \"test samples =>\" + str(num_samples_test))\n",
    "    \n",
    "    return filter, stride, filters,  train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test\n",
    "def process4(tissue, src_testing_data):\n",
    "    s = datetime.now(timezone('Asia/Bangkok')).strftime('log_date_%Y-%m-%d_time_%H-%M-%S')\n",
    "    \n",
    "    error_rates = []\n",
    "    sum_error_rate = 0\n",
    "    \n",
    "    src_root = \"../training_data/features/\"\n",
    "    src = src_root + tissue + \"/\"\n",
    "    \n",
    "    dst_root = \"../training_data/model/\"\n",
    "    dst = dst_root + tissue + \"/\"\n",
    "    \n",
    "    create_dir(dst_root)\n",
    "    create_dir(dst)\n",
    "    \n",
    "    path_log = dst + s  + \".txt\"\n",
    "    file_writer = open(path_log, 'w')\n",
    "    \n",
    "    for index in range(1,2):\n",
    "        src_k =  src + \"k-\" + str(index+1) + \"/\"\n",
    "        filter, stride, filters,  train_per_sweep, reader_train, reader_test, num_samples_train, num_samples_test = load_config3(src_k)\n",
    "        \n",
    "        \n",
    "        string_filter_details = \"folder => {:s}, train_per_sweep => {:d}\\nfilter => {:d}, stride => {:d}, dept => {:d}, {:d} {:d} {:d}\".format(src_k, train_per_sweep, filter, stride, len(filters), filters[0], filters[1], filters[2])\n",
    "        string_append_and_print(string_filter_details, path_log)\n",
    "        \n",
    "        error_rate, l, ls,e = do_train_test(reader_train, reader_test, input_dim, index+1, path_log, num_output_classes, filter, stride, filters, train_per_sweep, num_samples_train, num_samples_test, dst)\n",
    "        \n",
    "        segmentation().segment1(dst, \"model\"+str(error_rate)+\".model\", src_testing_data)\n",
    "    \n",
    "# process1(\"granulation\")\n",
    "src_testing_data = '../testing_data/granulation/'\n",
    "process4(\"augmen_motoC_to_s8_and_s8_to_iPad/granulation\", src_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
